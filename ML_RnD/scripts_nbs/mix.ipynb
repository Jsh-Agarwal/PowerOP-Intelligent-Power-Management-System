{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json  # Add missing json import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # Add missing seaborn import\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import shap\n",
    "import logging\n",
    "from multiprocessing import Pool\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Bidirectional, MultiHeadAttention, LayerNormalization, Dropout, Flatten\n",
    "from scikeras.wrappers import KerasRegressor  # New import\n",
    "from sklearn.base import BaseEstimator, RegressorMixin  # Add missing import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_output_directory(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    logging.basicConfig(filename=os.path.join(output_dir, 'application.log'), level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "    logging.info(\"Script started.\")\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "    \n",
    "    # Try different encodings\n",
    "    encodings = ['latin1', 'utf-8', 'cp1252']\n",
    "    data = None\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            data = pd.read_csv(file_path, encoding=encoding)\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    if data is None:\n",
    "        raise ValueError(\"Could not read file with any of the attempted encodings\")\n",
    "    data['Day'] = pd.to_datetime(data['Day'])\n",
    "    \n",
    "    # Print column names to debug\n",
    "    logging.info(f\"Original columns: {data.columns.tolist()}\")\n",
    "    \n",
    "    def normalize_column_name(col):\n",
    "        # Remove special characters and normalize spaces\n",
    "        col = col.strip()\n",
    "        col = col.replace('Ã‚', '')\n",
    "        col = col.replace('Â°', '')\n",
    "        col = col.replace('Âº', '')\n",
    "        col = col.replace('Ã‚ÂºC', 'C')\n",
    "        col = col.replace('Ã‚Â°C', 'C')\n",
    "        col = col.replace('Â°C', 'C')\n",
    "        col = col.replace('(C)', '(Â°C)')\n",
    "        return col\n",
    "\n",
    "    # Normalize all column names\n",
    "    data.columns = [normalize_column_name(col) for col in data.columns]\n",
    "\n",
    "    # Define expected column names\n",
    "    numerical_features = [\n",
    "        'Voltage (V)', 'Current (A)', 'Power Factor', 'Frequency (Hz)', 'Energy (kWh)',\n",
    "        'Inside Temperature (Â°C)', 'Outside Temperature (Â°C)', 'Inside Humidity (%)', 'Outside Humidity (%)'\n",
    "    ]\n",
    "    \n",
    "    # Create a comprehensive mapping for temperature columns\n",
    "    temp_variations = [\n",
    "        ('Inside Temperature(C)', 'Inside Temperature (Â°C)'),\n",
    "        ('Outside Temperature(C)', 'Outside Temperature (Â°C)'),\n",
    "        ('Inside Temperature (C)', 'Inside Temperature (Â°C)'),\n",
    "        ('Outside Temperature (C)', 'Outside Temperature (Â°C)'),\n",
    "        ('Inside Temperature', 'Inside Temperature (Â°C)'),\n",
    "        ('Outside Temperature', 'Outside Temperature (Â°C)'),\n",
    "        ('InsideTemp(C)', 'Inside Temperature (Â°C)'),\n",
    "        ('OutsideTemp(C)', 'Outside Temperature (Â°C)'),\n",
    "        ('Inside_Temperature', 'Inside Temperature (Â°C)'),\n",
    "        ('Outside_Temperature', 'Outside Temperature (Â°C)')\n",
    "    ]\n",
    "    \n",
    "    # Try to find and map temperature columns\n",
    "    for old_name, new_name in temp_variations:\n",
    "        if old_name in data.columns:\n",
    "            data = data.rename(columns={old_name: new_name})\n",
    "            logging.info(f\"Renamed column '{old_name}' to '{new_name}'\")\n",
    "    \n",
    "    # Function to find closest matching column\n",
    "    def find_closest_match(target, columns):\n",
    "        import difflib\n",
    "        matches = difflib.get_close_matches(normalize_column_name(target), \n",
    "                                          [normalize_column_name(col) for col in columns], \n",
    "                                          n=1, cutoff=0.7)\n",
    "        if matches:\n",
    "            orig_cols = [col for col in columns if normalize_column_name(col) == matches[0]]\n",
    "            return orig_cols[0] if orig_cols else None\n",
    "        return None\n",
    "\n",
    "    # Try to map required columns\n",
    "    for required_col in numerical_features:\n",
    "        if required_col not in data.columns:\n",
    "            match = find_closest_match(required_col, data.columns)\n",
    "            if match:\n",
    "                data = data.rename(columns={match: required_col})\n",
    "                logging.info(f\"Mapped '{match}' to required column '{required_col}'\")\n",
    "            else:\n",
    "                logging.warning(f\"Could not find match for required column: {required_col}\")\n",
    "\n",
    "    # Verify columns after mapping\n",
    "    missing_cols = [col for col in numerical_features if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        logging.error(f\"Missing columns after mapping: {missing_cols}\")\n",
    "        logging.error(f\"Available columns: {data.columns.tolist()}\")\n",
    "        # Create missing columns with NaN values instead of raising error\n",
    "        for col in missing_cols:\n",
    "            data[col] = np.nan\n",
    "            logging.warning(f\"Created missing column '{col}' with NaN values\")\n",
    "\n",
    "    # Use a separate scaler for the target variable\n",
    "    target_scaler = StandardScaler()\n",
    "    data['Power (W)'] = target_scaler.fit_transform(data[['Power (W)']])\n",
    "\n",
    "    # Impute missing values\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    data[numerical_features] = imputer.fit_transform(data[numerical_features])\n",
    "\n",
    "    # Feature Engineering\n",
    "    data['Temperature Difference'] = data['Outside Temperature (Â°C)'] - data['Inside Temperature (Â°C)']\n",
    "    data['Humidity Difference'] = data['Outside Humidity (%)'] - data['Inside Humidity (%)']\n",
    "    data['Power Factor * Current'] = data['Power Factor'] * data['Current (A)']\n",
    "    data['Current Squared'] = data['Current (A)'] ** 2\n",
    "\n",
    "    # Normalize input features\n",
    "    feature_scaler = StandardScaler()\n",
    "    feature_columns = numerical_features + ['Temperature Difference', 'Humidity Difference', 'Power Factor * Current', 'Current Squared']\n",
    "    data[feature_columns] = feature_scaler.fit_transform(data[feature_columns])\n",
    "    \n",
    "    return data, feature_scaler, target_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(X_train, y_train, features):\n",
    "    # Build LSTM model\n",
    "    def build_lstm_model():\n",
    "        input_layer = Input(shape=(1, len(features)))\n",
    "        lstm_1 = LSTM(64, return_sequences=True)(input_layer)\n",
    "        lstm_2 = LSTM(32, activation='relu')(lstm_1)\n",
    "        output_layer = Dense(1)(lstm_2)\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
    "        return model\n",
    "    lstm_model = build_lstm_model()\n",
    "\n",
    "    # Compile the model with the new optimizer\n",
    "    optimizer = Adam(learning_rate=0.01)\n",
    "    lstm_model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    # Define learning rate scheduler\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "    # Use EarlyStopping to optimize execution time\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping, lr_scheduler], verbose=1)\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Generate predictions and evaluate\n",
    "    predictions = model.predict(X_test)\n",
    "    # Reshape predictions to match y_test dimensions\n",
    "    predictions = predictions.reshape(-1)\n",
    "    y_test = y_test.values if hasattr(y_test, 'values') else y_test\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    performance_metrics = {\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(autoencoder, X, threshold):\n",
    "    # Compute reconstruction error and detect anomalies\n",
    "    X_pred = autoencoder.predict(X)\n",
    "    mse = np.mean(np.power(X - X_pred, 2), axis=1)\n",
    "    anomalies = mse > threshold\n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(data, output_dir):\n",
    "    # Plot power consumption trends\n",
    "    plt.figure()\n",
    "    plt.plot(data['Day'], data['Power (W)'])\n",
    "    plt.title('Power Consumption Over Time')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Power (W)')\n",
    "    plt.savefig(os.path.join(output_dir, 'power_trends.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot anomalies\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data['Day'], data['Power (W)'], label='Power Consumption')\n",
    "    anomalies = data[data['Anomaly'] == -1]\n",
    "    plt.scatter(anomalies['Day'], anomalies['Power (W)'], color='red', label='Anomalies')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Power (W)')\n",
    "    plt.title('Power Consumption Over Time with Anomalies')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'power_trends_with_anomalies.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_optimal_settings(lstm_model, autoencoder, temperature_range, current_range, threshold, feature_scaler, feature_columns, features):  # Add missing parameters\n",
    "    logging.info(\"Optimal settings prediction started.\")\n",
    "    \n",
    "    temp_grid, curr_grid = np.meshgrid(temperature_range, current_range)\n",
    "    temp_values = temp_grid.flatten()\n",
    "    curr_values = curr_grid.flatten()\n",
    "    \n",
    "    # Initialize new_data with the correct number of rows\n",
    "    n_samples = len(temp_values)\n",
    "    new_data = pd.DataFrame({\n",
    "        'Voltage (V)': np.full(n_samples, 220.0),\n",
    "        'Current (A)': curr_values,\n",
    "        'Voltage (V)': np.full(n_samples, 220.0),\n",
    "        'Current (A)': curr_values,\n",
    "        'Power Factor': np.full(n_samples, 0.9),\n",
    "        'Frequency (Hz)': np.full(n_samples, 50.0),\n",
    "        'Energy (kWh)': np.zeros(n_samples),\n",
    "        'Inside Temperature (Â°C)': temp_values,\n",
    "        'Outside Temperature (Â°C)': np.full(n_samples, 25.0),\n",
    "        'Inside Humidity (%)': np.full(n_samples, 50.0),\n",
    "        'Outside Humidity (%)': np.full(n_samples, 50.0),\n",
    "        'Temperature Difference': np.full(n_samples, 25.0) - temp_values,  # Calculate properly\n",
    "        'Humidity Difference': np.zeros(n_samples),  # Will be calculated\n",
    "        'Power Factor * Current': np.full(n_samples, 0.9) * curr_values,  # Calculate properly\n",
    "        'Current Squared': curr_values ** 2  # Calculate properly\n",
    "    })\n",
    "    \n",
    "    # Update calculated columns\n",
    "    new_data['Temperature Difference'] = new_data['Outside Temperature (Â°C)'] - new_data['Inside Temperature (Â°C)']\n",
    "    new_data['Humidity Difference'] = new_data['Outside Humidity (%)'] - new_data['Inside Humidity (%)']\n",
    "    new_data['Power Factor * Current'] = new_data['Power Factor'] * new_data['Current (A)']\n",
    "    new_data['Current Squared'] = new_data['Current (A)'] ** 2\n",
    "    \n",
    "    # Ensure column order matches feature_columns\n",
    "    new_data = new_data[feature_columns]\n",
    "    \n",
    "    # Transform the data using the scaler\n",
    "    try:\n",
    "        new_data_normalized = feature_scaler.transform(new_data)\n",
    "        new_data_normalized_df = pd.DataFrame(new_data_normalized, columns=feature_columns)\n",
    "        \n",
    "        # Prepare data for LSTM prediction\n",
    "        lstm_input = new_data_normalized_df[features].values.reshape((len(new_data), 1, len(features)))\n",
    "        \n",
    "        # Rest of the function remains the same\n",
    "        predicted_power_scaled = lstm_model.predict(lstm_input)\n",
    "        predicted_power = target_scaler.inverse_transform(predicted_power_scaled).flatten()\n",
    "        \n",
    "        # Prepare data for autoencoder\n",
    "        autoencoder_input = new_data_normalized_df[features].values\n",
    "        reconstruction = autoencoder.predict(autoencoder_input)\n",
    "        reconstruction_error = np.mean(np.power(autoencoder_input - reconstruction, 2), axis=1)\n",
    "        \n",
    "        # Create mask for valid conditions\n",
    "        valid_mask = ((new_data['Inside Temperature (Â°C)'] >= 21) & \n",
    "                     (new_data['Inside Temperature (Â°C)'] <= 25) & \n",
    "                     (reconstruction_error < threshold) &\n",
    "                     (new_data['Inside Humidity (%)'] >= 40) & \n",
    "                     (new_data['Inside Humidity (%)'] <= 60))\n",
    "        \n",
    "        # Find valid indices\n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        \n",
    "        if len(valid_indices) > 0:\n",
    "            # Find optimal settings among valid combinations\n",
    "            valid_powers = predicted_power[valid_indices]\n",
    "            min_power_idx = np.argmin(valid_powers)\n",
    "            optimal_settings = new_data.iloc[valid_indices[min_power_idx]]\n",
    "            min_power = predicted_power[valid_indices[min_power_idx]]\n",
    "            optimal_temp = optimal_settings['Inside Temperature (Â°C)']\n",
    "            logging.info(f\"Optimal settings found at temperature {optimal_temp}Â°C with power consumption {min_power} W.\")\n",
    "        else:\n",
    "            optimal_settings = None\n",
    "            min_power = None\n",
    "            optimal_temp = None\n",
    "            logging.warning(\"No optimal settings found within the specified ranges.\")\n",
    "        \n",
    "        logging.info(\"Optimal settings prediction completed.\")\n",
    "        \n",
    "        return optimal_settings, min_power, optimal_temp\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during optimization: {str(e)}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(metrics, optimal_settings, data, output_dir, cv_results=None, feature_importance=None):\n",
    "    # Generate Markdown report summarizing outputs\n",
    "    logging.info(\"Generating report.\")\n",
    "    try:\n",
    "        with open(os.path.join(output_dir, 'report.md'), 'w') as report_file:\n",
    "            report_file.write(\"# Analysis Report\\n\")\n",
    "            report_file.write(\"## Model Performance\\n\")\n",
    "            report_file.write(f\"- Mean Absolute Error (MAE): {metrics['mae']:.4f}\\n\")\n",
    "            report_file.write(f\"- Root Mean Squared Error (RMSE): {metrics['rmse']:.4f}\\n\")\n",
    "            report_file.write(f\"- RÂ² Score: {metrics['r2']:.4f}\\n\")\n",
    "            \n",
    "            report_file.write(\"\\n## Optimal Settings\\n\")\n",
    "            if optimal_settings is not None:\n",
    "                report_file.write(f\"- **Optimal Temperature:** {optimal_settings['Inside Temperature (Â°C)']}Â°C\\n\")\n",
    "                if 'min_power' in metrics:\n",
    "                    report_file.write(f\"- **Minimum Power Consumption:** {metrics['min_power']:.2f} W\\n\")\n",
    "                report_file.write(f\"- **Optimal Settings Details:**\\n\")\n",
    "                report_file.write(f\"```\\n{optimal_settings}\\n```\\n\")\n",
    "            else:\n",
    "                report_file.write(\"No optimal settings found within the specified ranges.\\n\")\n",
    "            \n",
    "            report_file.write(\"\\n## Anomalies Detected\\n\")\n",
    "            # Check for anomaly columns before accessing them\n",
    "            autoencoder_anomalies = data.get('Anomaly_Autoencoder', pd.Series([0] * len(data)))\n",
    "            isolation_forest_anomalies = data.get('Anomaly', pd.Series([0] * len(data)))\n",
    "            \n",
    "            report_file.write(f\"- Total anomalies detected by Autoencoder: {autoencoder_anomalies.sum()}\\n\")\n",
    "            report_file.write(f\"- Total anomalies detected by Isolation Forest: {(isolation_forest_anomalies == -1).sum()}\\n\")\n",
    "            \n",
    "            if 'Cluster' in data.columns:\n",
    "                report_file.write(\"\\n## Cluster Analysis\\n\")\n",
    "                report_file.write(\"- Cluster distribution:\\n\")\n",
    "                cluster_counts = data['Cluster'].value_counts()\n",
    "                for cluster_id, count in cluster_counts.items():\n",
    "                    report_file.write(f\"  - Cluster {cluster_id}: {count} instances\\n\")\n",
    "            \n",
    "            if cv_results:\n",
    "                report_file.write(\"\\n## Cross-Validation Results\\n\")\n",
    "                for model_name, metrics in cv_results.items():\n",
    "                    report_file.write(f\"### {model_name}\\n\")\n",
    "                    report_file.write(f\"- MAE: {metrics['mae_mean']:.4f} Â± {metrics['mae_std']:.4f}\\n\")\n",
    "                    report_file.write(f\"- RMSE: {metrics['rmse_mean']:.4f} Â± {metrics['rmse_std']:.4f}\\n\")\n",
    "                    report_file.write(f\"- RÂ²: {metrics['r2_mean']:.4f} Â± {metrics['r2_std']:.4f}\\n\")\n",
    "            \n",
    "            if feature_importance is not None:\n",
    "                report_file.write(\"\\n## Feature Importance\\n\")\n",
    "                report_file.write(feature_importance.to_string(index=False))\n",
    "            \n",
    "        logging.info(\"Report generated successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating report: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add these model architectures between the existing training functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bi_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(128, return_sequences=True, input_shape=input_shape)),\n",
    "        Bidirectional(LSTM(64, activation='relu')),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_attention_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    attention_layer = MultiHeadAttention(num_heads=4, key_dim=8)(input_layer, input_layer)\n",
    "    flatten_layer = Flatten()(attention_layer)\n",
    "    dense_layer = Dense(64, activation='relu')(flatten_layer)\n",
    "    output_layer = Dense(1)(dense_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_lstm_attention_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    lstm_layer = LSTM(64, return_sequences=True)(input_layer)\n",
    "    attention_layer = MultiHeadAttention(num_heads=4, key_dim=8)(lstm_layer, lstm_layer)\n",
    "    attention_normalized = LayerNormalization()(attention_layer)\n",
    "    dense_layer = Dense(64, activation='relu')(attention_normalized)\n",
    "    dropout_layer = Dropout(0.2)(dense_layer)\n",
    "    output_layer = Dense(1)(dropout_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(32, activation=\"relu\")(input_layer)\n",
    "    encoder = Dense(16, activation=\"relu\")(encoder)\n",
    "    encoder = Dropout(0.2)(encoder)\n",
    "    decoder = Dense(32, activation=\"relu\")(encoder)\n",
    "    decoder = Dense(input_dim, activation=\"linear\")(decoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hyperparameter_tuning(X_train, y_train):\n",
    "    def create_model(neurons=64, learning_rate=0.001):\n",
    "        model = Sequential([\n",
    "            LSTM(neurons, input_shape=(1, X_train.shape[2]), return_sequences=True),\n",
    "            LSTM(neurons // 2, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "        return model\n",
    "    class CustomLSTMRegressor(BaseEstimator, RegressorMixin):\n",
    "        def __init__(self, neurons=64, learning_rate=0.001, epochs=50, batch_size=32, verbose=0):\n",
    "            self.neurons = neurons\n",
    "            self.learning_rate = learning_rate\n",
    "            self.epochs = epochs\n",
    "            self.batch_size = batch_size\n",
    "            self.verbose = verbose\n",
    "            self.model = None\n",
    "        def fit(self, X, y):\n",
    "            self.model = create_model(\n",
    "                neurons=self.neurons,\n",
    "                learning_rate=self.learning_rate\n",
    "            )\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            self.model.fit(\n",
    "                X, y,\n",
    "                epochs=self.epochs,\n",
    "                batch_size=self.batch_size,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=self.verbose\n",
    "            )\n",
    "            return self\n",
    "        def predict(self, X):\n",
    "            if self.model is None:\n",
    "                raise RuntimeError(\"Model has not been fitted yet.\")\n",
    "            return self.model.predict(X).reshape(-1)\n",
    "        def score(self, X, y):\n",
    "            predictions = self.predict(X)\n",
    "            return -mean_squared_error(y, predictions)  # Negative MSE for scoring\n",
    "\n",
    "    # Create model wrapper\n",
    "    model = CustomLSTMRegressor()\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'neurons': [32, 64, 128],\n",
    "        'learning_rate': [0.01, 0.001, 0.0001],\n",
    "        'batch_size': [16, 32, 64]\n",
    "    }\n",
    "    \n",
    "    # Create GridSearchCV with proper configuration\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=TimeSeriesSplit(n_splits=3),\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Ensure input data is properly shaped\n",
    "    X_train_reshaped = X_train if len(X_train.shape) == 3 else X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    \n",
    "    # Perform grid search\n",
    "    try:\n",
    "        grid_result = grid.fit(X_train_reshaped, y_train)\n",
    "        logging.info(f\"Best parameters found: {grid_result.best_params_}\")\n",
    "        logging.info(f\"Best score: {grid_result.best_score_}\")\n",
    "        return grid_result\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Hyperparameter tuning failed: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename):\n",
    "    \"\"\"Remove or replace invalid characters in filenames\"\"\"\n",
    "    # Map of invalid characters to their replacements\n",
    "    invalid_chars = {\n",
    "        '<': '_lt_',\n",
    "        '>': '_gt_',\n",
    "        ':': '_',\n",
    "        '\"': '_',\n",
    "        '/': '_',\n",
    "        '\\\\': '_',\n",
    "        '|': '_',\n",
    "        '?': '_',\n",
    "        '*': '_x_',\n",
    "        ' ': '_',\n",
    "        '(': '_',\n",
    "        ')': '_',\n",
    "        '+': '_plus_',\n",
    "        'Â°': 'deg',\n",
    "        '%': 'pct'\n",
    "    }\n",
    "    \n",
    "    # Replace each invalid character\n",
    "    for char, replacement in invalid_chars.items():\n",
    "        filename = filename.replace(char, replacement)\n",
    "    \n",
    "    # Remove any remaining invalid characters\n",
    "    filename = ''.join(c for c in filename if c.isalnum() or c in '_-.')\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_shap_analysis(model, X_train, X_test, features, output_dir):\n",
    "    \"\"\"Perform SHAP analysis with proper file naming.\"\"\"\n",
    "    logging.info(\"Starting SHAP analysis.\")\n",
    "    try:\n",
    "        n_samples = 50\n",
    "        X_sample = shap.sample(X_test, n_samples)\n",
    "        X_train_summary = shap.kmeans(X_train, n_samples)\n",
    "        \n",
    "        def batch_predict(data):\n",
    "            data_reshaped = data.reshape((data.shape[0], 1, data.shape[1]))\n",
    "            return model.predict(data_reshaped).flatten()\n",
    "        \n",
    "        explainer = shap.KernelExplainer(batch_predict, X_train_summary)\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "        \n",
    "        # Generate SHAP plots with sanitized filenames\n",
    "        for feature in features:\n",
    "            feature_idx = features.index(feature)\n",
    "            safe_feature_name = sanitize_filename(feature)\n",
    "            \n",
    "            try:\n",
    "                shap.dependence_plot(\n",
    "                    feature_idx, \n",
    "                    shap_values,\n",
    "                    X_sample,\n",
    "                    feature_names=features,\n",
    "                    interaction_index=None,\n",
    "                    show=False\n",
    "                )\n",
    "                plt.savefig(os.path.join(output_dir, f'shap_dependence_{safe_feature_name}.png'))\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving SHAP plot for feature {feature}: {str(e)}\")\n",
    "                plt.close()\n",
    "        \n",
    "        logging.info(\"SHAP analysis completed successfully.\")\n",
    "        return shap_values\n",
    "    except Exception as e:\n",
    "        logging.error(f\"SHAP analysis failed: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_shap_analysis(model, X_train, X_test, features, output_dir):\n",
    "    \"\"\"Detailed SHAP analysis with multiple visualizations\"\"\"\n",
    "    try:\n",
    "        # Initialize SHAP analysis\n",
    "        n_samples = 50\n",
    "        X_sample = shap.sample(X_test, n_samples)\n",
    "        X_train_summary = shap.kmeans(X_train, n_samples)\n",
    "        \n",
    "        def batch_predict(data):\n",
    "            data_reshaped = data.reshape((data.shape[0], 1, data.shape[1]))\n",
    "            return model.predict(data_reshaped).flatten()\n",
    "        \n",
    "        # Create SHAP explainer and calculate values\n",
    "        explainer = shap.KernelExplainer(batch_predict, X_train_summary)\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "        \n",
    "        # Generate basic SHAP plots with sanitized filenames\n",
    "        for feature in features:\n",
    "            feature_idx = features.index(feature)\n",
    "            safe_feature_name = sanitize_filename(feature)\n",
    "            \n",
    "            try:\n",
    "                shap.dependence_plot(\n",
    "                    feature_idx, \n",
    "                    shap_values,\n",
    "                    X_sample,\n",
    "                    feature_names=features,\n",
    "                    interaction_index=None,\n",
    "                    show=False\n",
    "                )\n",
    "                plt.savefig(os.path.join(output_dir, f'shap_dependence_{safe_feature_name}.png'))\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving SHAP plot for feature {feature}: {str(e)}\")\n",
    "                plt.close()\n",
    "        \n",
    "        # Additional visualizations\n",
    "        try:\n",
    "            shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "            plt.savefig(os.path.join(output_dir, 'shap_summary_bar.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            shap.summary_plot(shap_values, X_sample, show=False)\n",
    "            plt.savefig(os.path.join(output_dir, 'shap_summary_dot.png'))\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving SHAP summary plots: {str(e)}\")\n",
    "            plt.close()\n",
    "        \n",
    "        # Feature importance ranking\n",
    "        feature_importance = np.abs(shap_values).mean(0)\n",
    "        importance_df = pd.DataFrame(list(zip(features, feature_importance)), \n",
    "                                   columns=['feature', 'importance'])\n",
    "        importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "        \n",
    "        return importance_df, shap_values\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Detailed SHAP analysis failed: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_advanced_lstm(features):  # Add features parameter\n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=(1, len(features))),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_complex_autoencoder(input_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    # Encoder\n",
    "    encoded = Dense(64, activation='relu')(input_layer)\n",
    "    encoded = LayerNormalization()(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "    encoded = Dropout(0.2)(encoded)\n",
    "    encoded = Dense(16, activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = LayerNormalization()(decoded)\n",
    "    decoded = Dense(64, activation='relu')(decoded)\n",
    "    decoded = Dropout(0.2)(decoded)\n",
    "    decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "    \n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_shap_analysis(model, X_train, X_test, features, output_dir):\n",
    "    \"\"\"Detailed SHAP analysis with multiple visualizations\"\"\"\n",
    "    # Initialize SHAP analysis\n",
    "    n_samples = 50\n",
    "    X_sample = shap.sample(X_test, n_samples)\n",
    "    X_train_summary = shap.kmeans(X_train, n_samples)\n",
    "    \n",
    "    def batch_predict(data):\n",
    "        data_reshaped = data.reshape((data.shape[0], 1, data.shape[1]))\n",
    "        return model.predict(data_reshaped).flatten()\n",
    "    \n",
    "    # Create SHAP explainer and calculate values\n",
    "    explainer = shap.KernelExplainer(batch_predict, X_train_summary)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # Generate basic SHAP plots\n",
    "    for feature in features:\n",
    "        feature_idx = features.index(feature)\n",
    "        shap.dependence_plot(\n",
    "            feature_idx, \n",
    "            shap_values,\n",
    "            X_sample,\n",
    "            feature_names=features,\n",
    "            interaction_index=None,\n",
    "            show=False\n",
    "        )\n",
    "        plt.savefig(os.path.join(output_dir, f'shap_dependence_{sanitize_filename(feature)}.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # Additional SHAP visualizations continue here...\n",
    "\n",
    "    # Additional SHAP visualizations\n",
    "    shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "    plt.savefig(os.path.join(output_dir, 'shap_summary_bar.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    shap.summary_plot(shap_values, X_sample, show=False)\n",
    "    plt.savefig(os.path.join(output_dir, 'shap_summary_dot.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature importance ranking\n",
    "    feature_importance = np.abs(shap_values).mean(0)\n",
    "    importance_df = pd.DataFrame(list(zip(features, feature_importance)), \n",
    "                               columns=['feature', 'importance'])\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "    \n",
    "    return importance_df, shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_cross_validation(models, X, y, n_splits=5):\n",
    "    \"\"\"Time series aware cross-validation with proper data reshaping\"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        metrics = {'mae': [], 'rmse': [], 'r2': []}\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            try:\n",
    "                # Get the split data\n",
    "                X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "                y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
    "                \n",
    "                # Ensure proper reshaping based on model type\n",
    "                if isinstance(model, Sequential) or isinstance(model, Model):\n",
    "                    # For LSTM and other sequential models\n",
    "                    if len(X_train_cv.shape) == 2:\n",
    "                        X_train_cv = X_train_cv.reshape((X_train_cv.shape[0], 1, X_train_cv.shape[1]))\n",
    "                        X_val_cv = X_val_cv.reshape((X_val_cv.shape[0], 1, X_val_cv.shape[1]))\n",
    "                    \n",
    "                    # Ensure y is properly shaped\n",
    "                    y_train_cv = np.array(y_train_cv).reshape(-1, 1)\n",
    "                    y_val_cv = np.array(y_val_cv).reshape(-1, 1)\n",
    "                    \n",
    "                    # Train the model\n",
    "                    model.fit(\n",
    "                        X_train_cv, \n",
    "                        y_train_cv,\n",
    "                        epochs=50,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_val_cv, y_val_cv),\n",
    "                        verbose=0,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=5)]\n",
    "                    )\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    predictions = model.predict(X_val_cv, verbose=0)\n",
    "                    \n",
    "                else:\n",
    "                    # For non-sequential models\n",
    "                    model.fit(X_train_cv, y_train_cv)\n",
    "                    predictions = model.predict(X_val_cv)\n",
    "                \n",
    "                # Ensure predictions and y_val_cv are 1D for metric calculation\n",
    "                predictions = predictions.ravel()\n",
    "                y_val_cv = y_val_cv.ravel()\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics['mae'].append(mean_absolute_error(y_val_cv, predictions))\n",
    "                metrics['rmse'].append(np.sqrt(mean_squared_error(y_val_cv, predictions)))\n",
    "                metrics['r2'].append(r2_score(y_val_cv, predictions))\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in cross-validation for model {name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate mean and std of metrics if we have any successful folds\n",
    "        if metrics['mae']:\n",
    "            cv_results[name] = {\n",
    "                'mae_mean': np.mean(metrics['mae']),\n",
    "                'mae_std': np.std(metrics['mae']),\n",
    "                'rmse_mean': np.mean(metrics['rmse']),\n",
    "                'rmse_std': np.std(metrics['rmse']),\n",
    "                'r2_mean': np.mean(metrics['r2']),\n",
    "                'r2_std': np.std(metrics['r2'])\n",
    "            }\n",
    "        else:\n",
    "            logging.warning(f\"No successful cross-validation folds for model {name}\")\n",
    "            cv_results[name] = {\n",
    "                'mae_mean': np.nan,\n",
    "                'mae_std': np.nan,\n",
    "                'rmse_mean': np.nan,\n",
    "                'rmse_std': np.nan,\n",
    "                'r2_mean': np.nan,\n",
    "                'r2_std': np.nan\n",
    "            }\n",
    "    \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_advanced_models():\n",
    "    \"\"\"Build all advanced model architectures\"\"\"\n",
    "    models = {\n",
    "        'lstm': Sequential([\n",
    "            LSTM(128, return_sequences=True, input_shape=(1, len(features))),\n",
    "            Dropout(0.2),\n",
    "            LSTM(64, return_sequences=True),\n",
    "            LSTM(32, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1)\n",
    "        ]),\n",
    "        'bi_lstm': Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            Bidirectional(LSTM(128, return_sequences=True)),\n",
    "            Bidirectional(LSTM(64, activation='relu')),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ]),\n",
    "        'attention': build_attention_model(input_shape),\n",
    "        'combined': build_combined_lstm_attention_model(input_shape)\n",
    "    }\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_shap_analysis(model, X_train, X_test, features, output_dir):\n",
    "    \"\"\"Detailed SHAP analysis with multiple visualizations\"\"\"\n",
    "    # ...existing SHAP code...\n",
    "    \n",
    "    # Additional SHAP visualizations and metrics\n",
    "    shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "    plt.savefig(os.path.join(output_dir, 'shap_summary_bar.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature importance matrix\n",
    "    feature_importance = np.abs(shap_values).mean(0)\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': feature_importance,\n",
    "        'abs_importance': np.abs(feature_importance)\n",
    "    }).sort_values('abs_importance', ascending=False)\n",
    "    \n",
    "    return importance_df, shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_cross_validation(models, X, y, n_splits=5):\n",
    "    \"\"\"Time series aware cross-validation\"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        metrics = {'mae': [], 'rmse': [], 'r2': []}\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            # ...existing cross-validation code...\n",
    "            X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "            y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
    "            \n",
    "            X_train_cv = X_train_cv.reshape((X_train_cv.shape[0], 1, X_train_cv.shape[1]))\n",
    "            X_val_cv = X_val_cv.reshape((X_val_cv.shape[0], 1, X_val_cv.shape[1]))\n",
    "            \n",
    "            model.fit(X_train_cv, y_train_cv, epochs=50, batch_size=32, \n",
    "                     validation_split=0.2, verbose=0,\n",
    "                     callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "            \n",
    "            predictions = model.predict(X_val_cv)\n",
    "            metrics['mae'].append(mean_absolute_error(y_val_cv, predictions))\n",
    "            metrics['rmse'].append(np.sqrt(mean_squared_error(y_val_cv, predictions)))\n",
    "            metrics['r2'].append(r2_score(y_val_cv, predictions))\n",
    "            \n",
    "            cv_results[name] = {\n",
    "                'mae_mean': np.mean(metrics['mae']),\n",
    "                'mae_std': np.std(metrics['mae']),\n",
    "                'rmse_mean': np.mean(metrics['rmse']),\n",
    "                'rmse_std': np.std(metrics['rmse']),\n",
    "                'r2_mean': np.mean(metrics['r2']),\n",
    "                'r2_std': np.std(metrics['r2'])\n",
    "            }\n",
    "    \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_complex_autoencoder(input_dim):\n",
    "    \"\"\"Enhanced autoencoder with additional layers and normalization\"\"\"\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    # Encoder\n",
    "    encoded = Dense(64, activation='relu')(input_layer)\n",
    "    encoded = LayerNormalization()(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "    encoded = Dropout(0.2)(encoded)\n",
    "    encoded = Dense(16, activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = LayerNormalization()(decoded)\n",
    "    decoded = Dense(64, activation='relu')(decoded)\n",
    "    decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "    \n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_visualization(data, shap_values, importance_df, output_dir, features):\n",
    "    \"\"\"Create comprehensive visualizations with proper error handling\"\"\"\n",
    "    try:\n",
    "        # Power consumption patterns\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(data['Day'], data['Power (W)'], label='Actual Power')\n",
    "        plt.title('Power Consumption Pattern Analysis')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Power (W)')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(output_dir, 'power_pattern.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Feature importance heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        pivot_table = importance_df.pivot_table(values='importance', index='feature')\n",
    "        sns.heatmap(pivot_table, annot=True, cmap='viridis', fmt='.3f')\n",
    "        plt.title('Feature Importance Heatmap')\n",
    "        plt.savefig(os.path.join(output_dir, 'feature_importance_heatmap.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # SHAP interaction plots\n",
    "        for feature in features:\n",
    "            try:\n",
    "                safe_feature_name = sanitize_filename(feature)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                feature_idx = features.index(feature)\n",
    "                shap.dependence_plot(\n",
    "                    feature_idx,\n",
    "                    shap_values,\n",
    "                    data[features],\n",
    "                    show=False,\n",
    "                    interaction_index=None\n",
    "                )\n",
    "                plt.title(f'SHAP Interaction Plot - {feature}')\n",
    "                plt.savefig(os.path.join(output_dir, f'shap_interaction_{safe_feature_name}.png'))\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving SHAP interaction plot for {feature}: {str(e)}\")\n",
    "                plt.close()\n",
    "        \n",
    "        # Anomaly distribution\n",
    "        if 'Reconstruction_error' in data.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(data['Reconstruction_error'], bins=50)\n",
    "            plt.axvline(x=np.percentile(data['Reconstruction_error'], 95),\n",
    "                       color='r', linestyle='--', label='Anomaly Threshold')\n",
    "            plt.title('Reconstruction Error Distribution')\n",
    "            plt.xlabel('Reconstruction Error')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(output_dir, 'anomaly_distribution.png'))\n",
    "            plt.close()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in advanced visualization: {str(e)}\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Execution Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    output_dir = setup_output_directory('New_output')\n",
    "    data, feature_scaler, target_scaler = load_and_preprocess_data('sensor_data.csv')\n",
    "    \n",
    "    # Prepare features and target variable\n",
    "    features = ['Current (A)', 'Inside Temperature (Â°C)', 'Temperature Difference',\n",
    "                'Humidity Difference', 'Power Factor * Current', 'Current Squared']\n",
    "    X = data[features]\n",
    "    y = data['Power (W)']\n",
    "\n",
    "    # Split data for supervised learning\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Reshape data for LSTM\n",
    "    X_train_lstm = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test_lstm = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    lstm_model = train_lstm_model(X_train_lstm, y_train, features)\n",
    "    performance_metrics = evaluate_model(lstm_model, X_test_lstm, y_test)\n",
    "\n",
    "    # Train multiple model architectures\n",
    "    bi_lstm_model = build_bi_lstm_model((1, len(features)))\n",
    "    attention_model = build_attention_model((1, len(features)))\n",
    "    combined_model = build_combined_lstm_attention_model((1, len(features)))\n",
    "    autoencoder = build_autoencoder(len(features))\n",
    "\n",
    "    # Train models\n",
    "    models = {\n",
    "        'bi_lstm': bi_lstm_model,\n",
    "        'attention': attention_model,\n",
    "        'combined': combined_model\n",
    "    }\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, \n",
    "                    validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "        metrics = evaluate_model(model, X_test_lstm, y_test)\n",
    "        logging.info(f\"{name} model metrics: {metrics}\")\n",
    "        \n",
    "    # Perform hyperparameter tuning\n",
    "    try:\n",
    "        best_params = perform_hyperparameter_tuning(X_train_lstm, y_train)\n",
    "        logging.info(f\"Best hyperparameters: {best_params.best_params_}\")\n",
    "        \n",
    "        # Create and train model with best parameters\n",
    "        best_model = create_model(**best_params.best_params_)\n",
    "        best_model.fit(\n",
    "            X_train_lstm, \n",
    "            y_train,\n",
    "            epochs=50,\n",
    "            batch_size=best_params.best_params_['batch_size'],\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during hyperparameter tuning: {str(e)}\")\n",
    "        # Continue with default model if tuning fails\n",
    "        best_model = lstm_model\n",
    "\n",
    "    # Modified SHAP analysis section\n",
    "    try:\n",
    "        shap_values = perform_shap_analysis(lstm_model, X_train, X_test, features, output_dir)\n",
    "        logging.info(\"SHAP analysis completed successfully\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"SHAP analysis failed: {str(e)}\")\n",
    "        shap_values = None\n",
    "    \n",
    "    # Initialize threshold with a default value\n",
    "    threshold = 0.1  # Default threshold value\n",
    "    \n",
    "    # Modified anomaly detection section\n",
    "    try:\n",
    "        autoencoder = build_autoencoder(len(features))\n",
    "        autoencoder.fit(X, X, epochs=50, batch_size=32, verbose=0)\n",
    "        \n",
    "        reconstruction_errors = autoencoder.predict(X)\n",
    "        reconstruction_error = np.mean(np.power(X - reconstruction_errors, 2), axis=1)\n",
    "        threshold = np.percentile(reconstruction_error, 95)  # Update threshold if successful\n",
    "        anomalies = detect_anomalies(autoencoder, X, threshold)\n",
    "        data['Anomaly'] = anomalies\n",
    "        data['Reconstruction_error'] = reconstruction_error\n",
    "        logging.info(f\"Anomaly detection threshold set to: {threshold}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Anomaly detection failed: {str(e)}\")\n",
    "        logging.warning(f\"Using default threshold value: {threshold}\")\n",
    "        anomalies = np.zeros(len(X))\n",
    "        data['Anomaly'] = 0\n",
    "        data['Reconstruction_error'] = np.zeros(len(X))\n",
    "\n",
    "    # Also ensure feature_columns is defined before optimization\n",
    "    feature_columns = features  # Use the same features list defined earlier\n",
    "\n",
    "    # Now proceed with optimization\n",
    "    temperature_range = np.arange(18, 30, 1)\n",
    "    current_range = np.arange(5, 15, 0.5)\n",
    "    optimal_settings, min_power, optimal_temp = predict_optimal_settings(\n",
    "        lstm_model, autoencoder, temperature_range, current_range, threshold, \n",
    "        feature_scaler, feature_columns, features\n",
    "    )\n",
    "\n",
    "    # Update metrics dictionary with min_power\n",
    "    performance_metrics['min_power'] = min_power\n",
    "\n",
    "    # Add Anomaly_Autoencoder column before report generation\n",
    "    data['Anomaly_Autoencoder'] = detect_anomalies(autoencoder, X, threshold)\n",
    "\n",
    "    # Report generation\n",
    "    generate_report(performance_metrics, optimal_settings, data, output_dir)\n",
    "    logging.info(\"Script completed successfully.\")\n",
    "\n",
    "    # Build and train advanced models\n",
    "    advanced_lstm = build_advanced_lstm(features)\n",
    "    complex_autoencoder = build_complex_autoencoder(len(features))\n",
    "    \n",
    "    # Perform detailed SHAP analysis\n",
    "    importance_df, shap_values = detailed_shap_analysis(lstm_model, X_train, X_test, features, output_dir)\n",
    "    \n",
    "    # Perform advanced cross-validation\n",
    "    models = {\n",
    "        'lstm': lstm_model,\n",
    "        'advanced_lstm': advanced_lstm,\n",
    "        'bi_lstm': bi_lstm_model,\n",
    "        'attention': attention_model,\n",
    "        'combined': combined_model\n",
    "    }\n",
    "    cv_results = advanced_cross_validation(models, X.values, y.values)\n",
    "    \n",
    "    # Update report generation to include new metrics\n",
    "    generate_report(performance_metrics, optimal_settings, data, output_dir, \n",
    "                   cv_results=cv_results, feature_importance=importance_df)\n",
    "    \n",
    "    # Save models and results\n",
    "    lstm_model.save(os.path.join(output_dir, 'lstm_model.keras'))\n",
    "    advanced_lstm.save(os.path.join(output_dir, 'advanced_lstm_model.keras'))\n",
    "    complex_autoencoder.save(os.path.join(output_dir, 'complex_autoencoder_model.keras'))\n",
    "    \n",
    "    # Save feature importance results\n",
    "    importance_df.to_csv(os.path.join(output_dir, 'feature_importance.csv'), index=False)\n",
    "    \n",
    "    # Save cross-validation results\n",
    "    with open(os.path.join(output_dir, 'cross_validation_results.json'), 'w') as f:\n",
    "        json.dump(cv_results, f, indent=4)\n",
    "    \n",
    "    # Create final visualizations\n",
    "    advanced_visualization(data, shap_values, importance_df, output_dir, features)\n",
    "    \n",
    "    # Save final processed data\n",
    "    data.to_csv(os.path.join(output_dir, 'processed_data.csv'), index=False)\n",
    "    \n",
    "    # Final logging\n",
    "    logging.info(\"All analyses completed successfully\")\n",
    "    logging.info(f\"Results saved to {output_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'models': models,\n",
    "        'performance_metrics': performance_metrics,\n",
    "        'cv_results': cv_results,\n",
    "        'feature_importance': importance_df,\n",
    "        'optimal_settings': optimal_settings,\n",
    "        'shap_values': shap_values\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
