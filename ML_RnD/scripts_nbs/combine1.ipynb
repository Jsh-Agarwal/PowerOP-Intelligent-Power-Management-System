{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Any, Tuple, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, Flatten, Bidirectional, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import holidays  # Ensure holidays module is imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all classes and functions from the individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainableAI:\n",
    "    def __init__(self, output_dir: str):\n",
    "        self.output_dir = output_dir\n",
    "        self.explainers = {}\n",
    "        self.shap_values = {}        \n",
    "    def _reshape_input_data(self, data: np.ndarray, model_type: str) -> np.ndarray:\n",
    "        \"\"\"Reshape input data based on model type\"\"\"\n",
    "        if model_type == 'lstm':\n",
    "            return data.reshape((data.shape[0], 1, -1))\n",
    "        return data\n",
    "    \n",
    "    def _create_model_wrapper(self, model, model_type: str):\n",
    "        \"\"\"Create prediction wrapper for different model types\"\"\"\n",
    "        def predict_wrapper(data):\n",
    "            reshaped_data = self._reshape_input_data(data, model_type)\n",
    "            if model_type in ['lstm', 'autoencoder']:\n",
    "                return model.predict(reshaped_data).flatten()\n",
    "            return model.predict(data)\n",
    "        return predict_wrapper\n",
    "    def create_explainer(self, \n",
    "                        model, \n",
    "                        X_train: np.ndarray,\n",
    "                        model_type: str,\n",
    "                        n_samples: int = 50) -> None:\n",
    "        \"\"\"Initialize SHAP explainer based on model type\"\"\"\n",
    "        X_background = shap.sample(X_train, n_samples)\n",
    "        predict_fn = self._create_model_wrapper(model, model_type)\n",
    "        \n",
    "        if model_type in ['lstm', 'autoencoder']:\n",
    "            self.explainers[model_type] = shap.KernelExplainer(\n",
    "                predict_fn, \n",
    "                shap.kmeans(X_background, n_samples)\n",
    "            )\n",
    "        else:\n",
    "            self.explainers[model_type] = shap.KernelExplainer(predict_fn, X_background)\n",
    "    def compute_shap_values(self, \n",
    "                          X_test: np.ndarray, \n",
    "                          model_type: str,\n",
    "                          n_samples: int = 50) -> np.ndarray:\n",
    "        \"\"\"Compute SHAP values for given model type\"\"\"\n",
    "        if model_type not in self.explainers:\n",
    "            raise ValueError(f\"No explainer found for model type: {model_type}\")\n",
    "            \n",
    "        X_sample = shap.sample(X_test, n_samples)\n",
    "        self.shap_values[model_type] = self.explainers[model_type].shap_values(X_sample)\n",
    "        return self.shap_values[model_type]\n",
    "    def generate_feature_importance(self, \n",
    "                                 model_type: str, \n",
    "                                 features: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Generate feature importance ranking\"\"\"\n",
    "        if model_type not in self.shap_values:\n",
    "            raise ValueError(f\"No SHAP values found for model type: {model_type}\")\n",
    "            \n",
    "        shap_vals = self.shap_values[model_type]\n",
    "        importance = np.abs(shap_vals).mean(0)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'importance': importance,\n",
    "            'abs_importance': np.abs(importance)\n",
    "        }).sort_values('abs_importance', ascending=False)\n",
    "    def plot_shap_summary(self, \n",
    "                         model_type: str, \n",
    "                         features: List[str], \n",
    "                         X_test: np.ndarray) -> None:\n",
    "        \"\"\"Generate and save SHAP summary plots\"\"\"\n",
    "        try:\n",
    "            # Bar summary\n",
    "            shap.summary_plot(\n",
    "                self.shap_values[model_type], \n",
    "                X_test,\n",
    "                feature_names=features,\n",
    "                plot_type=\"bar\",\n",
    "                show=False\n",
    "            )\n",
    "            plt.savefig(os.path.join(self.output_dir, f'shap_summary_bar_{model_type}.png'))\n",
    "            plt.close()\n",
    "\n",
    "            # Dot summary\n",
    "            shap.summary_plot(\n",
    "                self.shap_values[model_type], \n",
    "                X_test,\n",
    "                feature_names=features,\n",
    "                show=False\n",
    "            )\n",
    "            plt.savefig(os.path.join(self.output_dir, f'shap_summary_dot_{model_type}.png'))\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in SHAP summary plotting: {str(e)}\")\n",
    "            plt.close()\n",
    "    def plot_feature_dependence(self, \n",
    "                              model_type: str, \n",
    "                              features: List[str], \n",
    "                              X_test: np.ndarray) -> None:\n",
    "        \"\"\"Generate and save SHAP dependence plots for each feature\"\"\"\n",
    "        try:\n",
    "            for idx, feature in enumerate(features):\n",
    "                safe_name = ''.join(c for c in feature if c.isalnum() or c in '_- ')\n",
    "                shap.dependence_plot(\n",
    "                    idx,\n",
    "                    self.shap_values[model_type],\n",
    "                    X_test,\n",
    "                    feature_names=features,\n",
    "                    show=False\n",
    "                )\n",
    "                plt.savefig(os.path.join(\n",
    "                    self.output_dir, \n",
    "                    f'shap_dependence_{model_type}_{safe_name}.png'\n",
    "                ))\n",
    "                plt.close()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in dependence plotting: {str(e)}\")\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilder:\n",
    "    @staticmethod\n",
    "    def build_basic_lstm(input_shape, output_dim=12):  # Add output_dim parameter\n",
    "        input_layer = Input(shape=input_shape)\n",
    "        lstm_1 = LSTM(64, return_sequences=True)(input_layer)\n",
    "        lstm_2 = LSTM(32, activation='relu')(lstm_1)\n",
    "        output_layer = Dense(output_dim)(lstm_2)  # Match output dimension with target\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        return model\n",
    "    @staticmethod\n",
    "    def build_advanced_lstm(input_shape, output_dim=12):  # Add output_dim parameter\n",
    "        model = Sequential([\n",
    "            LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "            Dropout(0.2),\n",
    "            LSTM(64, return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(32, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(output_dim)  # Match output dimension with target\n",
    "        ])\n",
    "        return model\n",
    "    @staticmethod\n",
    "    def build_bi_lstm(input_shape, output_dim=12):  # Add output_dim parameter\n",
    "        \"\"\"Build Bidirectional LSTM model with correct input shape handling\"\"\"\n",
    "        # Flatten the middle two dimensions if we have a 4D input\n",
    "        if len(input_shape) == 3:\n",
    "            input_shape = (input_shape[0], input_shape[1] * input_shape[2])\n",
    "            \n",
    "        model = Sequential([\n",
    "            Input(shape=input_shape),  # Explicit input layer\n",
    "            Bidirectional(LSTM(128, return_sequences=True)),\n",
    "            Bidirectional(LSTM(64, activation='relu')),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(output_dim)\n",
    "        ])\n",
    "        return model\n",
    "    @staticmethod\n",
    "    def build_attention(input_shape, output_dim=12):  # Add output_dim parameter\n",
    "        \"\"\"Build Attention model with correct input shape handling\"\"\"\n",
    "        # Flatten the middle two dimensions if we have a 4D input\n",
    "        if len(input_shape) == 3:\n",
    "            input_shape = (input_shape[0], input_shape[1] * input_shape[2])\n",
    "            \n",
    "        input_layer = Input(shape=input_shape)\n",
    "        attention_layer = MultiHeadAttention(num_heads=4, key_dim=8)(input_layer, input_layer)\n",
    "        flatten_layer = Flatten()(attention_layer)\n",
    "        dense_layer = Dense(64, activation='relu')(flatten_layer)\n",
    "        output_layer = Dense(output_dim)(dense_layer)  # Match output dimension with target\n",
    "        return Model(inputs=input_layer, outputs=output_layer)\n",
    "    @staticmethod\n",
    "    def build_combined_lstm_attention(input_shape, output_dim=12):  # Add output_dim parameter\n",
    "        \"\"\"Build Combined LSTM-Attention model with correct input shape handling\"\"\"\n",
    "        # Flatten the middle two dimensions if we have a 4D input\n",
    "        if len(input_shape) == 3:\n",
    "            input_shape = (input_shape[0], input_shape[1] * input_shape[2])\n",
    "            \n",
    "        input_layer = Input(shape=input_shape)\n",
    "        lstm_layer = LSTM(64, return_sequences=True)(input_layer)\n",
    "        attention_layer = MultiHeadAttention(num_heads=4, key_dim=8)(lstm_layer, lstm_layer)\n",
    "        attention_normalized = LayerNormalization()(attention_layer)\n",
    "        flatten_layer = Flatten()(attention_normalized)\n",
    "        dense_layer = Dense(64, activation='relu')(flatten_layer)\n",
    "        dropout_layer = Dropout(0.2)(dense_layer)\n",
    "        output_layer = Dense(output_dim)(dropout_layer)  # Match output dimension with target\n",
    "        return Model(inputs=input_layer, outputs=output_layer)\n",
    "    @staticmethod\n",
    "    def build_model(input_shape):\n",
    "        model = Sequential()\n",
    "        # ...existing code...\n",
    "        model.add(Dense(24 * 12, activation='linear'))  # Adjust the output layer\n",
    "        model.add(layers.Reshape((24, 12)))  # Reshape to match the expected output shape\n",
    "        # ...existing code...\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, input_shape, output_dim=12, sequence_length=24):\n",
    "        \"\"\"Initialize ModelTrainer with correct input shape handling\"\"\"\n",
    "        # Ensure input_shape is correctly formatted\n",
    "        if len(input_shape) == 2:  # If given (timesteps, features)\n",
    "            self.input_shape = (input_shape[0], input_shape[1])\n",
    "        elif len(input_shape) == 3:  # If given (batch, timesteps, features)\n",
    "            self.input_shape = (input_shape[1], input_shape[2])\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input shape: {input_shape}\")\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.models = {\n",
    "            'bi_lstm': ModelBuilder.build_bi_lstm(self.input_shape, self.output_dim),\n",
    "            'attention': ModelBuilder.build_attention(self.input_shape, self.output_dim),\n",
    "            'combined': ModelBuilder.build_combined_lstm_attention(self.input_shape, self.output_dim)\n",
    "        }\n",
    "        self.sequence_length = sequence_length\n",
    "        self.reshape_required = True  # Enable reshaping for training/inference\n",
    "    def compile_models(self, learning_rate=0.001):\n",
    "        for name, model in self.models.items():\n",
    "            model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    def _prepare_input_data(self, X):\n",
    "        \"\"\"Prepare input data by reshaping if necessary\"\"\"\n",
    "        if self.reshape_required and len(X.shape) == 2:\n",
    "            # Split feature dimension into timesteps & features_per_timestep\n",
    "            # e.g., if X.shape[1] = 24*59, we reshape to (batch_size, 24, 59)\n",
    "            if X.shape[1] % self.sequence_length != 0:\n",
    "                raise ValueError(\"Feature dimension must be a multiple of sequence_length.\")\n",
    "            features_per_timestep = X.shape[1] // self.sequence_length\n",
    "            X = X.reshape((X.shape[0], self.sequence_length, features_per_timestep))\n",
    "        return X\n",
    "    def train_model(self, model, X_train, y_train, **kwargs):\n",
    "        \"\"\"Train a single model with proper input data preparation\"\"\"\n",
    "        X_train_prepared = self._prepare_input_data(X_train)\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "        ]\n",
    "        \n",
    "        return model.fit(\n",
    "            X_train_prepared, y_train,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            **kwargs\n",
    "        )\n",
    "    def train_all_models(self, X_train, y_train, epochs=50, batch_size=32):\n",
    "        results = {}\n",
    "        for name, model in self.models.items():\n",
    "            logging.info(f\"Training {name} model...\")\n",
    "            history = self.train_model(\n",
    "                model, X_train, y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                verbose=1\n",
    "            )\n",
    "            results[name] = history\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, feature_scaler=None, target_scaler=None):\n",
    "        self.feature_scaler = feature_scaler\n",
    "        self.target_scaler = target_scaler\n",
    "    @staticmethod\n",
    "    def evaluate_model(model, X_test, y_test):\n",
    "        predictions = model.predict(X_test)\n",
    "        predictions = predictions.reshape(-1)\n",
    "        y_test = y_test.values if hasattr(y_test, 'values') else y_test\n",
    "        \n",
    "        return {\n",
    "            'mae': mean_absolute_error(y_test, predictions),\n",
    "            'mse': mean_squared_error(y_test, predictions),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test, predictions)),\n",
    "            'r2': r2_score(y_test, predictions)\n",
    "        }\n",
    "    @staticmethod\n",
    "    def cross_validate(models, X, y, n_splits=5):\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        cv_results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            metrics = {'mae': [], 'rmse': [], 'r2': []}\n",
    "            \n",
    "            for train_idx, val_idx in tscv.split(X):\n",
    "                X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "                y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
    "                \n",
    "                # Ensure proper data shape\n",
    "                if len(X_train_cv.shape) == 2:\n",
    "                    X_train_cv = X_train_cv.reshape((X_train_cv.shape[0], 1, X_train_cv.shape[1]))\n",
    "                    X_val_cv = X_val_cv.reshape((X_val_cv.shape[0], 1, X_val_cv.shape[1]))\n",
    "                \n",
    "                # Train and evaluate\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "                model.fit(X_train_cv, y_train_cv, epochs=50, batch_size=32,\n",
    "                         validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                val_metrics = ModelEvaluator.evaluate_model(model, X_val_cv, y_val_cv)\n",
    "                metrics['mae'].append(val_metrics['mae'])\n",
    "                metrics['rmse'].append(val_metrics['rmse'])\n",
    "                metrics['r2'].append(val_metrics['r2'])\n",
    "            \n",
    "            # Calculate average metrics\n",
    "            cv_results[name] = {\n",
    "                'mae_mean': np.mean(metrics['mae']),\n",
    "                'mae_std': np.std(metrics['mae']),\n",
    "                'rmse_mean': np.mean(metrics['rmse']),\n",
    "                'rmse_std': np.std(metrics['rmse']),\n",
    "                'r2_mean': np.mean(metrics['r2']),\n",
    "                'r2_std': np.std(metrics['r2'])\n",
    "            }\n",
    "            \n",
    "        return cv_results\n",
    "    def find_optimal_settings(self, model, autoencoder, features, feature_columns):\n",
    "        \"\"\"\n",
    "        Find optimal temperature and current settings for minimum power consumption\n",
    "        while maintaining comfort conditions.\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary containing:\n",
    "            - optimal_settings: DataFrame row with optimal parameters\n",
    "            - min_power: Minimum predicted power consumption\n",
    "            - optimal_temp: Optimal temperature\n",
    "            - optimization_space: DataFrame with all evaluated combinations\n",
    "        \"\"\"\n",
    "        # Define search spaces\n",
    "        temperature_range = np.arange(18, 30, 1)\n",
    "        current_range = np.arange(5, 15, 0.5)\n",
    "        temp_grid, curr_grid = np.meshgrid(temperature_range, current_range)\n",
    "        \n",
    "        # Create parameter combinations\n",
    "        settings_df = pd.DataFrame({\n",
    "            'Inside Temperature (Â°C)': temp_grid.flatten(),\n",
    "            'Current (A)': curr_grid.flatten()\n",
    "        })\n",
    "        \n",
    "        # Add constant parameters\n",
    "        settings_df['Voltage (V)'] = 220.0\n",
    "        settings_df['Power Factor'] = 0.9\n",
    "        settings_df['Outside Temperature (Â°C)'] = 25.0\n",
    "        settings_df['Inside Humidity (%)'] = 50.0\n",
    "        settings_df['Outside Humidity (%)'] = 50.0\n",
    "        \n",
    "        # Calculate derived features\n",
    "        settings_df['Temperature Difference'] = (\n",
    "            settings_df['Outside Temperature (Â°C)'] - \n",
    "            settings_df['Inside Temperature (Â°C)']\n",
    "        )\n",
    "        settings_df['Power Factor * Current'] = (\n",
    "            settings_df['Power Factor'] * settings_df['Current (A)']\n",
    "        )\n",
    "        settings_df['Current Squared'] = settings_df['Current (A)'] ** 2\n",
    "        \n",
    "        # Normalize features\n",
    "        if self.feature_scaler:\n",
    "            normalized_data = self.feature_scaler.transform(settings_df[feature_columns])\n",
    "            normalized_df = pd.DataFrame(normalized_data, columns=feature_columns)\n",
    "        else:\n",
    "            normalized_df = settings_df[feature_columns]\n",
    "        \n",
    "        # Prepare input for prediction\n",
    "        model_input = normalized_df[features].values.reshape((-1, 1, len(features)))\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model.predict(model_input)\n",
    "        if self.target_scaler:\n",
    "            predictions = self.target_scaler.inverse_transform(predictions)\n",
    "        \n",
    "        # Get anomaly scores if autoencoder is provided\n",
    "        if autoencoder is not None:\n",
    "            reconstruction = autoencoder.predict(normalized_df[features].values)\n",
    "            reconstruction_error = np.mean(np.power(\n",
    "                normalized_df[features].values - reconstruction, 2\n",
    "            ), axis=1)\n",
    "            settings_df['anomaly_score'] = reconstruction_error\n",
    "        \n",
    "        # Add predictions to results\n",
    "        settings_df['predicted_power'] = predictions.flatten()\n",
    "        \n",
    "        # Apply comfort constraints\n",
    "        valid_mask = (\n",
    "            (settings_df['Inside Temperature (Â°C)'] >= 21) & \n",
    "            (settings_df['Inside Temperature (Â°C)'] <= 25) & \n",
    "            (settings_df['Inside Humidity (%)'] >= 40) & \n",
    "            (settings_df['Inside Humidity (%)'] <= 60)\n",
    "        )\n",
    "        \n",
    "        if autoencoder is not None:\n",
    "            threshold = np.percentile(settings_df['anomaly_score'], 95)\n",
    "            valid_mask &= (settings_df['anomaly_score'] < threshold)\n",
    "        \n",
    "        # Find optimal settings\n",
    "        valid_settings = settings_df[valid_mask]\n",
    "        if len(valid_settings) > 0:\n",
    "            optimal_idx = valid_settings['predicted_power'].idxmin()\n",
    "            optimal_settings = valid_settings.loc[optimal_idx]\n",
    "            min_power = optimal_settings['predicted_power']\n",
    "            optimal_temp = optimal_settings['Inside Temperature (Â°C)']\n",
    "        else:\n",
    "            optimal_settings = None\n",
    "            min_power = None\n",
    "            optimal_temp = None\n",
    "        \n",
    "        return {\n",
    "            'optimal_settings': optimal_settings,\n",
    "            'min_power': min_power,\n",
    "            'optimal_temp': optimal_temp,\n",
    "            'optimization_space': settings_df\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HVACDataPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive data preprocessing pipeline for HVAC system data.\n",
    "    Handles missing values, feature engineering, and data validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 scaler_type: str = 'standard',\n",
    "                 imputer_n_neighbors: int = 5,\n",
    "                 country_holidays: str = 'US'):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor with configuration parameters.\n",
    "        \n",
    "        Args:\n",
    "            scaler_type: Type of scaler ('standard' or 'minmax')\n",
    "            imputer_n_neighbors: Number of neighbors for KNN imputation\n",
    "            country_holidays: Country code for holiday detection\n",
    "        \"\"\"\n",
    "        self.scaler_type = scaler_type\n",
    "        self.scaler = StandardScaler() if scaler_type == 'standard' else MinMaxScaler()\n",
    "        self.imputer = KNNImputer(n_neighbors=imputer_n_neighbors)\n",
    "        self.holidays = holidays.CountryHoliday(country_holidays)\n",
    "        self.feature_names = None\n",
    "        self.numerical_columns = None\n",
    "        self._setup_logging()\n",
    "        \n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configure logging for the preprocessor\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    def _validate_raw_data(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Validate the input data structure and content.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "        Raises:\n",
    "            ValueError: If data validation fails\n",
    "        \"\"\"\n",
    "        required_columns = [\n",
    "            'Date', 'on_off', 'damper', 'active_energy', 'co2_1', 'amb_humid_1',\n",
    "            'active_power', 'pot_gen', 'high_pressure_1', 'high_pressure_2',\n",
    "            'low_pressure_1', 'low_pressure_2', 'high_pressure_3', 'low_pressure_3',\n",
    "            'outside_temp', 'outlet_temp', 'inlet_temp', 'summer_setpoint_temp',\n",
    "            'winter_setpoint_temp', 'amb_temp_2'\n",
    "        ]\n",
    "        \n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "            \n",
    "        # Check data types\n",
    "        try:\n",
    "            pd.to_datetime(df['Date'])\n",
    "        except:\n",
    "            raise ValueError(\"Date column cannot be parsed as datetime\")\n",
    "    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle missing values using appropriate strategies.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "        Returns:\n",
    "            DataFrame with handled missing values\n",
    "        \"\"\"\n",
    "        # Fill boolean columns\n",
    "        boolean_columns = ['on_off', 'damper']\n",
    "        df[boolean_columns] = df[boolean_columns].fillna(0)\n",
    "        \n",
    "        # Handle numerical columns with KNN imputation\n",
    "        numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_columns] = self.imputer.fit_transform(df[numerical_columns])\n",
    "        \n",
    "        return df\n",
    "    def _engineer_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create time-based features from the Date column.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "        Returns:\n",
    "            DataFrame with additional time-based features\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        # Extract basic time components\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['is_weekend'] = df['datetime'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "        \n",
    "        # Create cyclical time features\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "        \n",
    "        # Add holiday indicator\n",
    "        df['is_holiday'] = df['datetime'].apply(lambda x: x in self.holidays).astype(int)\n",
    "        \n",
    "        return df\n",
    "    def _engineer_hvac_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create HVAC-specific engineered features.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "        Returns:\n",
    "            DataFrame with additional HVAC-specific features\n",
    "        \"\"\"\n",
    "        # Temperature differences\n",
    "        df['temp_difference_in_out'] = df['outlet_temp'] - df['inlet_temp']\n",
    "        df['temp_difference_ambient'] = df['outside_temp'] - df['inlet_temp']\n",
    "        \n",
    "        # Pressure ratios and differences\n",
    "        df['high_pressure_avg'] = df[['high_pressure_1', 'high_pressure_2', 'high_pressure_3']].mean(axis=1)\n",
    "        df['low_pressure_avg'] = df[['low_pressure_1', 'low_pressure_2', 'low_pressure_3']].mean(axis=1)\n",
    "        df['pressure_ratio'] = df['high_pressure_avg'] / df['low_pressure_avg']\n",
    "        \n",
    "        # System efficiency indicators\n",
    "        df['power_per_temp_diff'] = df['active_power'] / (df['temp_difference_in_out'] + 1e-6)\n",
    "        df['energy_efficiency'] = df['active_energy'] / (df['active_power'] + 1e-6)\n",
    "        \n",
    "        # Comfort indicators\n",
    "        df['temp_setpoint_diff'] = np.where(\n",
    "            df['month'].isin([6, 7, 8]),  # Summer months\n",
    "            df['inlet_temp'] - df['summer_setpoint_temp'],\n",
    "            df['inlet_temp'] - df['winter_setpoint_temp']\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    def _create_rolling_features(self, df: pd.DataFrame, windows: List[int] = [3, 6, 12]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create rolling window features for key metrics.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            windows: List of window sizes (in hours) for rolling calculations\n",
    "        Returns:\n",
    "            DataFrame with additional rolling features\n",
    "        \"\"\"\n",
    "        key_metrics = ['active_power', 'inlet_temp', 'co2_1', 'amb_humid_1']\n",
    "        \n",
    "        for window in windows:\n",
    "            for metric in key_metrics:\n",
    "                # Rolling mean\n",
    "                df[f'{metric}_rolling_mean_{window}h'] = (\n",
    "                    df[metric].rolling(window=window * 12, min_periods=1).mean()\n",
    "                )\n",
    "                # Rolling std\n",
    "                df[f'{metric}_rolling_std_{window}h'] = (\n",
    "                    df[metric].rolling(window=window * 12, min_periods=1).std()\n",
    "                )\n",
    "        \n",
    "        return df\n",
    "    def _prepare_target_variable(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Prepare the target variable (active_power) for modeling.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "        Returns:\n",
    "            Tuple of (features DataFrame, target Series)\n",
    "        \"\"\"\n",
    "        target = df['active_power']\n",
    "        features = df.drop(['active_power', 'datetime', 'Date'], axis=1)\n",
    "        \n",
    "        return features, target\n",
    "    def _scale_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scale numerical features using the configured scaler.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "        Returns:\n",
    "            DataFrame with scaled features\n",
    "        \"\"\"\n",
    "        numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_columns] = self.scaler.fit_transform(df[numerical_columns])\n",
    "        return df\n",
    "    def preprocess(self, \n",
    "                  df: pd.DataFrame, \n",
    "                  training: bool = True) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Execute the complete preprocessing pipeline.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            training: Whether this is for training data (True) or inference (False)\n",
    "        Returns:\n",
    "            Tuple of (preprocessed features, target variable)\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting preprocessing pipeline...\")\n",
    "        \n",
    "        # Validate raw data\n",
    "        self._validate_raw_data(df)\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = self._handle_missing_values(df)\n",
    "        \n",
    "        # Engineer features\n",
    "        df = self._engineer_time_features(df)\n",
    "        df = self._engineer_hvac_features(df)\n",
    "        df = self._create_rolling_features(df)\n",
    "        \n",
    "        # Prepare features and target\n",
    "        features, target = self._prepare_target_variable(df)\n",
    "        \n",
    "        # Scale features\n",
    "        features = self._scale_features(features)\n",
    "        \n",
    "        if training:\n",
    "            self.feature_names = features.columns.tolist()\n",
    "            self.numerical_columns = features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        self.logger.info(\"Preprocessing pipeline completed successfully.\")\n",
    "        return features, target\n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        \"\"\"Return the list of feature names after preprocessing\"\"\"\n",
    "        if self.feature_names is None:\n",
    "            raise ValueError(\"Preprocessor hasn't been fitted with training data yet\")\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    \"\"\"\n",
    "    Validates data quality and generates reports on data issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_data_quality(df: pd.DataFrame) -> Dict[str, Union[float, List[str]]]:\n",
    "        \"\"\"\n",
    "        Perform comprehensive data quality checks.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "        Returns:\n",
    "            Dictionary containing quality metrics and issues\n",
    "        \"\"\"\n",
    "        quality_report = {\n",
    "            'missing_values': {},\n",
    "            'outliers': {},\n",
    "            'inconsistencies': [],\n",
    "            'data_coverage': {}\n",
    "        }\n",
    "        \n",
    "        # Check missing values\n",
    "        missing_vals = df.isnull().sum()\n",
    "        quality_report['missing_values'] = {\n",
    "            col: count for col, count in missing_vals.items() if count > 0\n",
    "        }\n",
    "        \n",
    "        # Check value ranges\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            q1 = df[col].quantile(0.25)\n",
    "            q3 = df[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            outliers = df[\n",
    "                (df[col] < (q1 - 1.5 * iqr)) | \n",
    "                (df[col] > (q3 + 1.5 * iqr))\n",
    "            ]\n",
    "            if len(outliers) > 0:\n",
    "                quality_report['outliers'][col] = len(outliers)\n",
    "        \n",
    "        # Check data consistency\n",
    "        if 'datetime' in df.columns:\n",
    "            time_gaps = df['datetime'].diff().dt.total_seconds() / 60\n",
    "            irregular_intervals = time_gaps[time_gaps != 5].index\n",
    "            if len(irregular_intervals) > 0:\n",
    "                quality_report['inconsistencies'].append(\n",
    "                    f\"Irregular time intervals found at {len(irregular_intervals)} points\"\n",
    "                )\n",
    "        \n",
    "        # Check data coverage\n",
    "        if 'datetime' in df.columns:\n",
    "            date_range = df['datetime'].max() - df['datetime'].min()\n",
    "            expected_records = (date_range.total_seconds() / 300) + 1  # 5-minute intervals\n",
    "            coverage = len(df) / expected_records * 100\n",
    "            quality_report['data_coverage'] = {\n",
    "                'start_date': df['datetime'].min(),\n",
    "                'end_date': df['datetime'].max(),\n",
    "                'coverage_percentage': coverage\n",
    "            }\n",
    "        \n",
    "        return quality_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector:\n",
    "    def __init__(self, input_dim, method='autoencoder'):\n",
    "        self.input_dim = input_dim\n",
    "        self.method = method\n",
    "        self.model = self._build_model()\n",
    "        self.threshold = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        if self.method == 'autoencoder':\n",
    "            return self._build_basic_autoencoder()\n",
    "        elif self.method == 'complex_autoencoder':\n",
    "            return self._build_complex_autoencoder()\n",
    "        elif self.method == 'isolation_forest':\n",
    "            return IsolationForest(contamination=0.1, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {self.method}\")\n",
    "    def _build_basic_autoencoder(self):\n",
    "        input_layer = Input(shape=(self.input_dim,))\n",
    "        # Encoder\n",
    "        encoder = Dense(32, activation=\"relu\")(input_layer)\n",
    "        encoder = Dense(16, activation=\"relu\")(encoder)\n",
    "        encoder = Dropout(0.2)(encoder)\n",
    "        # Decoder\n",
    "        decoder = Dense(32, activation=\"relu\")(encoder)\n",
    "        decoder = Dense(self.input_dim, activation=\"linear\")(decoder)\n",
    "        \n",
    "        model = Model(inputs=input_layer, outputs=decoder)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    def _build_complex_autoencoder(self):\n",
    "        input_layer = Input(shape=(self.input_dim,))\n",
    "        # Enhanced encoder\n",
    "        encoded = Dense(64, activation='relu')(input_layer)\n",
    "        encoded = LayerNormalization()(encoded)\n",
    "        encoded = Dense(32, activation='relu')(encoded)\n",
    "        encoded = Dropout(0.2)(encoded)\n",
    "        encoded = Dense(16, activation='relu')(encoded)\n",
    "        encoded = LayerNormalization()(encoded)\n",
    "        \n",
    "        # Enhanced decoder\n",
    "        decoded = Dense(32, activation='relu')(encoded)\n",
    "        decoded = LayerNormalization()(decoded)\n",
    "        decoded = Dense(64, activation='relu')(decoded)\n",
    "        decoded = Dropout(0.2)(decoded)\n",
    "        decoded = LayerNormalization()(decoded)\n",
    "        decoded = Dense(self.input_dim, activation='linear')(decoded)\n",
    "        \n",
    "        model = Model(inputs=input_layer, outputs=decoded)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    def fit(self, X, epochs=50, batch_size=32):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        if isinstance(self.model, Model):  # Autoencoder models\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            \n",
    "            history = self.model.fit(\n",
    "                X_scaled, X_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Set threshold based on reconstruction error\n",
    "            predictions = self.model.predict(X_scaled)\n",
    "            reconstruction_errors = np.mean(np.power(X_scaled - predictions, 2), axis=1)\n",
    "            self.threshold = np.percentile(reconstruction_errors, 95)\n",
    "            \n",
    "            return history\n",
    "        else:  # Isolation Forest\n",
    "            self.model.fit(X_scaled)\n",
    "            return None\n",
    "    def detect_anomalies(self, X, threshold=None):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        if isinstance(self.model, Model):  # Autoencoder models\n",
    "            predictions = self.model.predict(X_scaled)\n",
    "            reconstruction_errors = np.mean(np.power(X_scaled - predictions, 2), axis=1)\n",
    "            threshold = threshold or self.threshold\n",
    "            return {\n",
    "                'anomalies': reconstruction_errors > threshold,\n",
    "                'scores': reconstruction_errors,\n",
    "                'threshold': threshold\n",
    "            }\n",
    "        else:  # Isolation Forest\n",
    "            predictions = self.model.predict(X_scaled)\n",
    "            return {\n",
    "                'anomalies': predictions == -1,\n",
    "                'scores': self.model.score_samples(X_scaled),\n",
    "                'threshold': None\n",
    "            }\n",
    "    def get_reconstruction(self, X):\n",
    "        \"\"\"Get reconstructed data for autoencoder models\"\"\"\n",
    "        if not isinstance(self.model, Model):\n",
    "            raise ValueError(\"Reconstruction only available for autoencoder models\")\n",
    "        \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        reconstructed = self.model.predict(X_scaled)\n",
    "        return self.scaler.inverse_transform(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetectorEnsemble:\n",
    "    def __init__(self, input_dim):\n",
    "        self.detectors = {\n",
    "            'basic_autoencoder': AnomalyDetector(input_dim, 'autoencoder'),\n",
    "            'complex_autoencoder': AnomalyDetector(input_dim, 'complex_autoencoder'),\n",
    "            'isolation_forest': AnomalyDetector(input_dim, 'isolation_forest')\n",
    "        }\n",
    "        \n",
    "    def fit(self, X, epochs=50, batch_size=32):\n",
    "        results = {}\n",
    "        for name, detector in self.detectors.items():\n",
    "            results[name] = detector.fit(X, epochs, batch_size)\n",
    "        return results\n",
    "    \n",
    "    def detect_anomalies(self, X):\n",
    "        results = {}\n",
    "        for name, detector in self.detectors.items():\n",
    "            results[name] = detector.detect_anomalies(X)\n",
    "        return results\n",
    "    \n",
    "    def get_ensemble_predictions(self, X, voting='majority'):\n",
    "        all_predictions = self.detect_anomalies(X)\n",
    "        \n",
    "        if voting == 'majority':\n",
    "            # Stack predictions from all detectors\n",
    "            predictions = np.column_stack([\n",
    "                pred['anomalies'] for pred in all_predictions.values()\n",
    "            ])\n",
    "            # Return True if majority of detectors flag as anomaly\n",
    "            return np.mean(predictions, axis=1) > 0.5\n",
    "        \n",
    "        return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HVACAnalysisPipeline:\n",
    "    \"\"\"\n",
    "    Main pipeline class that coordinates data processing, model training,\n",
    "    and result generation for HVAC system analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with configuration parameters.\n",
    "        \n",
    "        Args:\n",
    "            config: Dictionary containing configuration parameters\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.setup_directories()\n",
    "        self.setup_logging()\n",
    "        \n",
    "    def setup_directories(self) -> None:\n",
    "        \"\"\"Create necessary directories for outputs\"\"\"\n",
    "        # Create timestamp-based output directory\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.output_dir = Path(self.config['output_base_dir']) / f\"analysis_{timestamp}\"\n",
    "        \n",
    "        # Create subdirectories\n",
    "        self.subdirs = {\n",
    "            'models': self.output_dir / 'models',\n",
    "            'plots': self.output_dir / 'plots',\n",
    "            'results': self.output_dir / 'results',\n",
    "            'anomalies': self.output_dir / 'anomalies',\n",
    "            'shap': self.output_dir / 'shap'\n",
    "        }\n",
    "        \n",
    "        for dir_path in self.subdirs.values():\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "    def setup_logging(self) -> None:\n",
    "        \"\"\"Configure logging for the pipeline\"\"\"\n",
    "        log_file = self.output_dir / 'pipeline.log'\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load data from CSV file and perform initial validation.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame containing raw data\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Loading data from {self.config['data_path']}\")\n",
    "        try:\n",
    "            df = pd.read_csv(self.config['data_path'])\n",
    "            self.logger.info(f\"Loaded {len(df)} records\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def save_results(self, results: Dict[str, Any], filename: str) -> None:\n",
    "        \"\"\"Save results to JSON file\"\"\"\n",
    "        file_path = self.subdirs['results'] / filename\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(results, f, indent=4, default=str)\n",
    "            \n",
    "    def plot_model_performance(self, results: Dict[str, Any]) -> None:\n",
    "        \"\"\"Generate and save model performance plots\"\"\"\n",
    "        # Training history plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for model_name, history in results['training_history'].items():\n",
    "            plt.plot(history.history['loss'], label=f'{model_name}_train')\n",
    "            plt.plot(history.history['val_loss'], label=f'{model_name}_val')\n",
    "        plt.title('Model Training History')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(self.subdirs['plots'] / 'training_history.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Model comparison plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        metrics = pd.DataFrame(results['evaluation_results']).T\n",
    "        metrics['r2'].plot(kind='bar')\n",
    "        plt.title('Model RÂ² Scores')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.subdirs['plots'] / 'model_comparison.png')\n",
    "        plt.close()\n",
    "        \n",
    "    def run_pipeline(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the complete analysis pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing all results and metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. Load and preprocess data\n",
    "            self.logger.info(\"Starting data preprocessing\")\n",
    "            raw_data = self.load_data()\n",
    "            \n",
    "            # Validate data quality\n",
    "            quality_report = DataValidator.check_data_quality(raw_data)\n",
    "            self.save_results(quality_report, 'data_quality_report.json')\n",
    "            \n",
    "            # Process data\n",
    "            X_processed, y = process_hvac_data(self.config['data_path'])\n",
    "            \n",
    "            # 2. Split data\n",
    "            split_idx = int(len(X_processed) * 0.8)\n",
    "            X_train, X_test = X_processed[:split_idx], X_processed[split_idx:]\n",
    "            y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "            \n",
    "            # Preprocess data\n",
    "            X_train, y_train = preprocess_data(X_train, y_train)\n",
    "            \n",
    "            # 3. Train regression models\n",
    "            self.logger.info(\"Training regression models\")\n",
    "            trainer = ModelTrainer(\n",
    "                input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                output_dim=y_train.shape[1]  # Add output dimension\n",
    "            )\n",
    "            trainer.compile_models()\n",
    "            training_results = trainer.train_all_models(X_train, y_train)\n",
    "            \n",
    "            # 4. Evaluate models\n",
    "            self.logger.info(\"Evaluating models\")\n",
    "            evaluator = ModelEvaluator()\n",
    "            evaluation_results = {}\n",
    "            for name, model in trainer.models.items():\n",
    "                evaluation_results[name] = evaluator.evaluate_model(model, X_test, y_test)\n",
    "            \n",
    "            # 5. Detect anomalies\n",
    "            self.logger.info(\"Performing anomaly detection\")\n",
    "            anomaly_detector = AnomalyDetectorEnsemble(X_train.shape[2])\n",
    "            anomaly_detector.fit(X_train)\n",
    "            anomaly_results = anomaly_detector.detect_anomalies(X_test)\n",
    "            \n",
    "            # 6. Generate SHAP explanations\n",
    "            self.logger.info(\"Generating model explanations\")\n",
    "            explainer = ExplainableAI(str(self.subdirs['shap']))\n",
    "            shap_results = analyze_model_explanability(\n",
    "                trainer.models,\n",
    "                X_train,\n",
    "                X_test,\n",
    "                self.config['feature_names']\n",
    "            )\n",
    "            \n",
    "            # 7. Compile and save results\n",
    "            results = {\n",
    "                'training_history': training_results,\n",
    "                'evaluation_results': evaluation_results,\n",
    "                'anomaly_results': anomaly_results,\n",
    "                'shap_results': shap_results\n",
    "            }\n",
    "            \n",
    "            self.save_results(results, 'analysis_results.json')\n",
    "            \n",
    "            # 8. Generate plots\n",
    "            self.plot_model_performance(results)\n",
    "            \n",
    "            # 9. Generate recommendations\n",
    "            recommendations = self.generate_recommendations(results)\n",
    "            self.save_results(recommendations, 'recommendations.json')\n",
    "            \n",
    "            self.logger.info(\"Pipeline completed successfully\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def generate_recommendations(self, results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate actionable recommendations based on analysis results.\n",
    "        \n",
    "        Args:\n",
    "            results: Dictionary containing analysis results\n",
    "        Returns:\n",
    "            Dictionary containing recommendations\n",
    "        \"\"\"\n",
    "        recommendations = {\n",
    "            'model_selection': {},\n",
    "            'anomaly_detection': {},\n",
    "            'system_optimization': {}\n",
    "        }\n",
    "        \n",
    "        # Model selection recommendations\n",
    "        best_model = max(\n",
    "            results['evaluation_results'].items(),\n",
    "            key=lambda x: x[1]['r2']\n",
    "        )[0]\n",
    "        \n",
    "        recommendations['model_selection'] = {\n",
    "            'best_model': best_model,\n",
    "            'performance_metrics': results['evaluation_results'][best_model],\n",
    "            'reason': f\"Selected based on highest RÂ² score of {results['evaluation_results'][best_model]['r2']:.3f}\"\n",
    "        }\n",
    "        \n",
    "        # Anomaly detection recommendations\n",
    "        anomaly_count = sum(results['anomaly_results']['ensemble_predictions'])\n",
    "        recommendations['anomaly_detection'] = {\n",
    "            'anomaly_count': int(anomaly_count),\n",
    "            'anomaly_percentage': float(anomaly_count / len(results['anomaly_results']['ensemble_predictions']) * 100),\n",
    "            'suggestion': \"Investigate system behavior during identified anomaly periods\"\n",
    "        }\n",
    "        \n",
    "        # System optimization recommendations\n",
    "        if 'optimization_results' in results:\n",
    "            opt = results['optimization_results']\n",
    "            recommendations['system_optimization'] = {\n",
    "                'optimal_temperature': float(opt['optimal_temp']),\n",
    "                'estimated_savings': float(opt['min_power']),\n",
    "                'implementation_steps': [\n",
    "                    \"Gradually adjust setpoints to optimal values\",\n",
    "                    \"Monitor system performance during transition\",\n",
    "                    \"Validate energy savings after implementation\"\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, y, sequence_length=24, forecast_horizon=12):\n",
    "    \"\"\"\n",
    "    Preprocess time series data into sequences for training.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features array (2D: samples, features)\n",
    "        y: Target values array (1D)\n",
    "        sequence_length: Length of input sequences\n",
    "        forecast_horizon: Number of future steps to predict\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X sequences, y sequences)\n",
    "    \"\"\"\n",
    "    # Ensure y is 1D\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.values\n",
    "    if len(y.shape) > 1:\n",
    "        y = y.ravel()\n",
    "    \n",
    "    # Calculate valid number of sequences\n",
    "    num_samples = len(y) - sequence_length - forecast_horizon + 1\n",
    "    \n",
    "    if num_samples <= 0:\n",
    "        raise ValueError(\"Not enough samples to create sequences\")\n",
    "    \n",
    "    # Initialize arrays with correct shapes\n",
    "    num_features = X.shape[-1]\n",
    "    X_sequences = np.zeros((num_samples, sequence_length, num_features))\n",
    "    y_sequences = np.zeros((num_samples, forecast_horizon))\n",
    "    \n",
    "    # Create sequences without reshaping input\n",
    "    for i in range(num_samples):\n",
    "        X_sequences[i] = X[i:i + sequence_length]\n",
    "        y_sequences[i] = y[i + sequence_length:i + sequence_length + forecast_horizon]\n",
    "    \n",
    "    return X_sequences, y_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_explanability(\n",
    "    models: Dict[str, BaseEstimator],\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    features: List[str],\n",
    "    output_dir: str\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of model explanability for both supervised and unsupervised models.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing feature importance and SHAP values for each model\n",
    "    \"\"\"\n",
    "    explainer = ExplainableAI(output_dir)\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            # Create explainer and compute SHAP values\n",
    "            explainer.create_explainer(model, X_train, model_name)\n",
    "            shap_values = explainer.compute_shap_values(X_test, model_name)\n",
    "            \n",
    "            # Generate importance analysis\n",
    "            importance_df = explainer.generate_feature_importance(model_name, features)\n",
    "            \n",
    "            # Create visualizations\n",
    "            explainer.plot_shap_summary(model_name, features, X_test)\n",
    "            explainer.plot_feature_dependence(model_name, features, X_test)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'shap_values': shap_values,\n",
    "                'feature_importance': importance_df\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error analyzing model {model_name}: {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_data(features: pd.DataFrame, \n",
    "                         target: pd.Series,\n",
    "                         sequence_length: int = 24,\n",
    "                         forecast_horizon: int = 12) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepare sequential data for time series models.\n",
    "    \n",
    "    Args:\n",
    "        features: Preprocessed feature DataFrame\n",
    "        target: Target variable Series\n",
    "        sequence_length: Number of time steps in each sequence\n",
    "        forecast_horizon: Number of steps to forecast\n",
    "    Returns:\n",
    "        Tuple of (X sequences, y sequences)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(features) - sequence_length - forecast_horizon + 1):\n",
    "        X.append(features.iloc[i:(i + sequence_length)].values)\n",
    "        y.append(target.iloc[i + sequence_length:i + sequence_length + forecast_horizon].values)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hyperparameter_tuning(X_train, y_train, input_shape):\n",
    "    \"\"\"Single implementation of hyperparameter tuning\"\"\"\n",
    "    \n",
    "    class CustomLSTMRegressor(BaseEstimator, RegressorMixin):\n",
    "        def __init__(self, neurons=64, learning_rate=0.001, epochs=50, batch_size=32):\n",
    "            self.neurons = neurons\n",
    "            self.learning_rate = learning_rate\n",
    "            self.epochs = epochs\n",
    "            self.batch_size = batch_size\n",
    "            self.model = None\n",
    "        def fit(self, X, y):\n",
    "            self.model = ModelBuilder.build_basic_lstm(input_shape)\n",
    "            self.model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "            self.model.fit(\n",
    "                X, y,\n",
    "                epochs=self.epochs,\n",
    "                batch_size=self.batch_size,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[EarlyStopping(monitor='val_loss', patience=5)],\n",
    "                verbose=0\n",
    "            )\n",
    "            return self\n",
    "        def predict(self, X):\n",
    "            return self.model.predict(X).reshape(-1)\n",
    "    param_grid = {\n",
    "        'neurons': [32, 64, 128],\n",
    "        'learning_rate': [0.01, 0.001, 0.0001],\n",
    "        'batch_size': [16, 32, 64]\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        estimator=CustomLSTMRegressor(),\n",
    "        param_grid=param_grid,\n",
    "        cv=TimeSeriesSplit(n_splits=3),\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=1\n",
    "    )\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    return grid_result.best_params_, grid_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_anomalies(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Analyze anomalies using multiple methods and compare results.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing results from all methods and ensemble predictions\n",
    "    \"\"\"\n",
    "    ensemble = AnomalyDetectorEnsemble(X_train.shape[1])\n",
    "    \n",
    "    # Train all detectors\n",
    "    training_results = ensemble.fit(X_train)\n",
    "    \n",
    "    # Get predictions from all methods\n",
    "    predictions = ensemble.detect_anomalies(X_test)\n",
    "    \n",
    "    # Get ensemble predictions\n",
    "    ensemble_predictions = ensemble.get_ensemble_predictions(X_test)\n",
    "    \n",
    "    return {\n",
    "        'individual_results': predictions,\n",
    "        'ensemble_predictions': ensemble_predictions,\n",
    "        'training_results': training_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hvac_data(data_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Process HVAC data from raw file to model-ready format.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to raw data file\n",
    "    Returns:\n",
    "        Tuple of (processed features, target variable)\n",
    "    \"\"\"\n",
    "    # Read data\n",
    "    df = pd.read_csv(data_path, parse_dates=['Date'])\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = HVACDataPreprocessor(\n",
    "        scaler_type='standard',\n",
    "        imputer_n_neighbors=5\n",
    "    )\n",
    "    \n",
    "    # Validate data quality\n",
    "    quality_report = DataValidator.check_data_quality(df)\n",
    "    logging.info(f\"Data quality report: {quality_report}\")\n",
    "    \n",
    "    # Preprocess data\n",
    "    features, target = preprocessor.preprocess(df, training=True)\n",
    "    \n",
    "    # Convert target to 1D array if needed\n",
    "    if isinstance(target, pd.Series):\n",
    "        target = target.values\n",
    "    if len(target.shape) > 1:\n",
    "        target = target.ravel()\n",
    "    \n",
    "    # Convert features to numpy array if it's a DataFrame\n",
    "    if isinstance(features, pd.DataFrame):\n",
    "        features = features.values\n",
    "        \n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test, features, feature_scaler, target_scaler):\n",
    "    # Initialize evaluator\n",
    "    evaluator = ModelEvaluator(feature_scaler, target_scaler)\n",
    "    \n",
    "    # Train models\n",
    "    trainer = ModelTrainer((1, len(features)))\n",
    "    trainer.compile_models()\n",
    "    training_results = trainer.train_all_models(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    evaluation_results = {}\n",
    "    for name, model in trainer.models.items():\n",
    "        metrics = evaluator.evaluate_model(model, X_test, y_test)\n",
    "        evaluation_results[name] = metrics\n",
    "    \n",
    "    # Find optimal settings for best model\n",
    "    best_model_name = max(evaluation_results, key=lambda k: evaluation_results[k]['r2'])\n",
    "    best_model = trainer.models[best_model_name]\n",
    "    \n",
    "    # Build autoencoder for anomaly detection\n",
    "    autoencoder = ModelBuilder.build_autoencoder(len(features))\n",
    "    autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Get optimal settings\n",
    "    optimization_results = evaluator.find_optimal_settings(\n",
    "        best_model, \n",
    "        autoencoder,\n",
    "        features,\n",
    "        feature_columns\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'models': trainer.models,\n",
    "        'evaluation_results': evaluation_results,\n",
    "        'optimization_results': optimization_results,\n",
    "        'best_model_name': best_model_name\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
