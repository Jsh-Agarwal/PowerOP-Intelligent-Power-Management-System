{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import pandas as pd\n", "import numpy as np\n", "from datetime import datetime\n", "import logging\n", "from pathlib import Path\n", "import json\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from typing import Dict, Any, Tuple, List, Union"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.base import BaseEstimator, RegressorMixin\n", "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n", "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n", "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n", "from sklearn.impute import KNNImputer\n", "from sklearn.ensemble import IsolationForest"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import tensorflow as tf\n", "from tensorflow.keras.models import Sequential, Model, load_model\n", "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, Flatten, Bidirectional, MultiHeadAttention, LayerNormalization\n", "from tensorflow.keras.optimizers import Adam\n", "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import shap\n", "import holidays"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define all classes and functions from the individual files"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from multiprocessing import Pool\n", "import psutil\n", "from tqdm import tqdm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ExplainableAI:\n", "    def __init__(self, output_dir: str):\n", "        self.output_dir = output_dir\n", "        self.explainers = {}\n", "        self.shap_values = {}\n", "        self.verbose = True\n", "        self.n_background_samples = 100  # Reduce background samples for speed\n", "    def _reshape_input_data(self, data: np.ndarray, model_type: str) -> np.ndarray:\n", "        if model_type == 'lstm':\n", "            return data.reshape((data.shape[0], 1, -1))\n", "        return data\n", "    \n", "    def _create_model_wrapper(self, model, model_type: str):\n", "        def predict_wrapper(data):\n", "            reshaped_data = self._reshape_input_data(data, model_type)\n", "            return model.predict(reshaped_data)\n", "        return predict_wrapper\n", "    def create_explainer(self, model, X_train: np.ndarray, model_type: str, n_samples: int = 50) -> None:\n", "        if self.verbose:\n", "            print(f\"Creating SHAP explainer for {model_type}...\")\n", "        \n", "        # Use smaller background dataset\n", "        X_background = shap.sample(X_train, min(self.n_background_samples, n_samples))\n", "        predict_fn = self._create_model_wrapper(model, model_type)\n", "        \n", "        if model_type in ['lstm', 'autoencoder']:\n", "            self.explainers[model_type] = shap.DeepExplainer(predict_fn, X_background)\n", "        else:\n", "            self.explainers[model_type] = shap.KernelExplainer(\n", "                predict_fn, \n", "                X_background,\n", "                nsamples=100  # Reduce samples for speed\n", "            )\n", "    def compute_shap_values(self, X_test: np.ndarray, model_type: str, n_samples: int = 50) -> np.ndarray:\n", "        if model_type not in self.explainers:\n", "            raise ValueError(f\"No explainer found for {model_type}\")\n", "        X_sample = shap.sample(X_test, n_samples)\n", "        self.shap_values[model_type] = self.explainers[model_type].shap_values(X_sample)\n", "        return self.shap_values[model_type]\n", "    def generate_feature_importance(self, model_type: str, features: list) -> pd.DataFrame:\n", "        if model_type not in self.shap_values:\n", "            raise ValueError(f\"No shap values computed for {model_type}\")\n", "        shap_vals = self.shap_values[model_type]\n", "        importance = np.abs(shap_vals).mean(0)\n", "        return pd.DataFrame({\n", "            'feature': features,\n", "            'importance': importance,\n", "            'abs_importance': np.abs(importance)\n", "        }).sort_values('abs_importance', ascending=False)\n", "    def plot_shap_summary(self, model_type: str, features: list, X_test: np.ndarray) -> None:\n", "        try:\n", "            shap.summary_plot(self.shap_values.get(model_type), X_test, feature_names=features, show=False)\n", "            plt.savefig(f\"{self.output_dir}/shap_summary_{model_type}.png\")\n", "            plt.close()\n", "        except Exception as e:\n", "            logging.error(f\"Error plotting shap summary: {e}\")\n", "    def plot_feature_dependence(self, model_type: str, features: list, X_test: np.ndarray) -> None:\n", "        try:\n", "            # Plot dependence for first feature as an example.\n", "            shap.dependence_plot(features[0], self.shap_values.get(model_type), X_test, show=False)\n", "            plt.savefig(f\"{self.output_dir}/shap_dependence_{features[0]}.png\")\n", "            plt.close()\n", "        except Exception as e:\n", "            logging.error(f\"Error plotting feature dependence: {e}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ModelBuilder:\n", "    @staticmethod\n", "    def build_basic_lstm(input_shape, output_dim=12):\n", "        # Minimal implementation\n", "        model = Sequential()\n", "        model.add(LSTM(64, input_shape=input_shape))\n", "        model.add(Dense(output_dim))\n", "        model.compile(optimizer=Adam(), loss='mse')\n", "        return model\n", "    @staticmethod\n", "    def build_advanced_lstm(input_shape, output_dim=12):\n", "        model = Sequential()\n", "        model.add(LSTM(128, return_sequences=True, input_shape=input_shape))\n", "        model.add(Dropout(0.2))\n", "        model.add(LSTM(64))\n", "        model.add(Dense(32, activation='relu'))\n", "        model.add(Dense(output_dim))\n", "        model.compile(optimizer=Adam(), loss='mse')\n", "        return model\n", "    @staticmethod\n", "    def build_bi_lstm(input_shape, output_dim=12):\n", "        model = Sequential()\n", "        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape))\n", "        model.add(Bidirectional(LSTM(64)))\n", "        model.add(Dense(output_dim))\n", "        model.compile(optimizer=Adam(), loss='mse')\n", "        return model\n", "    @staticmethod\n", "    def build_attention(input_shape, output_dim=12):\n", "        inputs = Input(shape=input_shape)\n", "        lstm_out = LSTM(128, return_sequences=True)(inputs)\n", "        attention_out = MultiHeadAttention(num_heads=2, key_dim=2)(lstm_out, lstm_out)\n", "        lstm_out = LSTM(64)(attention_out)\n", "        output = Dense(output_dim)(lstm_out)\n", "        model = tf.keras.Model(inputs=inputs, outputs=output)\n", "        model.compile(optimizer=Adam(), loss='mse')\n", "        return model\n", "    @staticmethod\n", "    def build_combined_lstm_attention(input_shape, output_dim=12):\n", "        return ModelBuilder.build_attention(input_shape, output_dim)\n", "    @staticmethod\n", "    def build_model(input_shape):\n", "        return ModelBuilder.build_basic_lstm(input_shape)\n", "    @staticmethod\n", "    def build_autoencoder(input_dim, encoding_dim=16):\n", "        inputs = Input(shape=(input_dim,))\n", "        encoded = Dense(encoding_dim, activation='relu')(inputs)\n", "        decoded = Dense(input_dim, activation='linear')(encoded)\n", "        autoencoder = Model(inputs=inputs, outputs=decoded)\n", "        autoencoder.compile(optimizer=Adam(), loss='mse')\n", "        return autoencoder"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ModelTrainer:\n", "    def __init__(self, input_shape, output_dim=12, sequence_length=24, model_dir=None):\n", "        if len(input_shape) == 2:\n", "            self.input_shape = (input_shape[0], input_shape[1])\n", "        elif len(input_shape) == 3:\n", "            self.input_shape = (input_shape[1], input_shape[2])\n", "        else:\n", "            raise ValueError(f\"Unexpected input shape: {input_shape}\")\n", "        \n", "        self.output_dim = output_dim\n", "        self.models = {\n", "            'bi_lstm': ModelBuilder.build_bi_lstm(self.input_shape, self.output_dim),\n", "            'attention': ModelBuilder.build_attention(self.input_shape, self.output_dim),\n", "            'combined': ModelBuilder.build_combined_lstm_attention(self.input_shape, self.output_dim)\n", "        }\n", "        self.sequence_length = sequence_length\n", "        self.reshape_required = True\n", "        self.model_dir = model_dir or Path(\"D:/PowerAmp/models\")  # Common directory for models\n", "        self.pretrained_model_dir = Path(\"D:/PowerAmp/outputs/analysis_20250215_175946/models\")  # Pretrained models directory\n", "    def compile_models(self, learning_rate=0.0005):  # Lower learning rate\n", "        for name, model in self.models.items():\n", "            model.compile(\n", "                optimizer=Adam(learning_rate=learning_rate),\n", "                loss='mse',\n", "                metrics=['mae']  # Changed metric to MAE\n", "            )\n", "    def load_existing_models(self):\n", "        if self.pretrained_model_dir:\n", "            for name in self.models.keys():\n", "                model_path = self.pretrained_model_dir / f\"{name}.keras\"\n", "                if model_path.exists():\n", "                    self.models[name] = load_model(model_path)\n", "                    logging.info(f\"Loaded existing model {name} from {model_path}\")\n", "    def _prepare_input_data(self, X):\n", "        if self.reshape_required and len(X.shape) == 2:\n", "            return X.reshape((X.shape[0], 1, X.shape[1]))\n", "        return X\n", "    def save_training_history(self, history, model_name):\n", "        history_df = pd.DataFrame(history.history)\n", "        history_df.to_csv(f\"{self.model_dir}/{model_name}_history.csv\", index=False)\n", "    def train_model(self, model, X_train, y_train, epochs=60, batch_size=32, **kwargs):  # Adjusted epochs and batch size\n", "        X_train_prepared = self._prepare_input_data(np.asarray(X_train, dtype='float32'))\n", "        callbacks = [\n", "            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),  # Adjusted patience\n", "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8)\n", "        ]\n", "        history = model.fit(\n", "            X_train_prepared, np.asarray(y_train, dtype='float32'),\n", "            validation_split=0.2,\n", "            callbacks=callbacks,\n", "            epochs=epochs,\n", "            batch_size=batch_size,\n", "            **kwargs\n", "        )\n", "        logging.info(\"Validation MAE: %s\", history.history.get('val_mae', ['N/A'])[-1])\n", "        self.save_training_history(history, model.name)\n", "        return history\n", "    def train_all_models(self, X_train, y_train, epochs=50, batch_size=32):  # Adjusted epochs and batch size\n", "        results = {}\n", "        for name, model in self.models.items():\n", "            model_path = self.pretrained_model_dir / f\"{name}.keras\"\n", "            if model_path.exists():\n", "                logging.info(f\"Loading existing model: {name}\")\n", "                self.models[name] = load_model(model_path)\n", "            else:\n", "                logging.info(f\"Training model: {name}\")\n", "                self.train_model(model, X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n", "                save_path = self.model_dir / f\"{name}.keras\"\n", "                if not save_path.exists():\n", "                    model.save(save_path)\n", "                    logging.info(f\"Saved model {name} to {save_path}\")\n", "            results[name] = {\"trained\": True}\n", "        return results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ModelEvaluator:\n", "    def __init__(self, feature_scaler=None, target_scaler=None):\n", "        self.feature_scaler = feature_scaler\n", "        self.target_scaler = target_scaler\n", "    @staticmethod\n", "    def evaluate_model(model, X_test, y_test):\n", "        \"\"\"\n", "        Evaluate model performance ensuring correct data shapes and handling NaN values.\n", "        \"\"\"\n", "        # Get predictions - expect X_test to be (samples, timesteps, features)\n", "        predictions = model.predict(X_test)\n", "        \n", "        # Reshape predictions and y_test to 1D arrays\n", "        predictions = predictions.reshape(-1)\n", "        y_test_flat = y_test.reshape(-1)[:predictions.shape[0]]\n", "        \n", "        # Remove NaN values from both arrays\n", "        mask = ~(np.isnan(predictions) | np.isnan(y_test_flat))\n", "        predictions_clean = predictions[mask]\n", "        y_test_clean = y_test_flat[mask]\n", "        \n", "        if len(predictions_clean) == 0:\n", "            logging.warning(\"No valid predictions after removing NaN values\")\n", "            return {\n", "                'mae': np.nan,\n", "                'mse': np.nan,\n", "                'rmse': np.nan,\n", "                'r2': np.nan,\n", "                'test_loss': np.nan,\n", "                'test_acc': np.nan,\n", "                'n_samples': 0,\n", "                'n_nans': np.sum(~mask)\n", "            }\n", "        \n", "        # Calculate metrics\n", "        mae = mean_absolute_error(y_test_clean, predictions_clean)\n", "        mse = mean_squared_error(y_test_clean, predictions_clean)\n", "        rmse = np.sqrt(mse)\n", "        r2 = r2_score(y_test_clean, predictions_clean)\n", "        \n", "        # Model evaluation with cleaned data\n", "        try:\n", "            loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n", "        except ValueError:\n", "            loss, test_acc = -1, -1\n", "            logging.warning(\"Could not calculate test accuracy due to shape mismatch\")\n", "        \n", "        return {\n", "            'mae': mae,\n", "            'mse': mse,\n", "            'rmse': rmse,\n", "            'r2': r2,\n", "            'test_loss': loss,\n", "            'test_acc': test_acc,\n", "            'n_samples': len(predictions_clean),\n", "            'n_nans': np.sum(~mask)\n", "        }"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class HVACDataPreprocessor:\n", "    def __init__(self, scaler_type: str = 'standard', imputer_n_neighbors: int = 5, country_holidays: str = 'US'):\n", "        self.scaler_type = scaler_type\n", "        self.scaler = StandardScaler() if scaler_type == 'standard' else MinMaxScaler()\n", "        self.imputer = KNNImputer(n_neighbors=imputer_n_neighbors)\n", "        self.holidays = holidays.CountryHoliday(country_holidays)\n", "        self.feature_names = None\n", "        self.numerical_columns = None\n", "        self._setup_logging()\n", "        \n", "    def _setup_logging(self):\n", "        logging.basicConfig(\n", "            level=logging.INFO,\n", "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'  # Fixed format string\n", "        )\n", "        self.logger = logging.getLogger(__name__)\n", "    def _validate_raw_data(self, df: pd.DataFrame) -> None:\n", "        required_columns = [\n", "            'Date', 'on_off', 'damper', 'active_energy', 'co2_1', 'amb_humid_1',\n", "            'active_power', 'pot_gen', 'high_pressure_1', 'high_pressure_2',\n", "            'low_pressure_1', 'low_pressure_2', 'high_pressure_3', 'low_pressure_3',\n", "            'outside_temp', 'outlet_temp', 'inlet_temp', 'summer_setpoint_temp',\n", "            'winter_setpoint_temp', 'amb_temp_2'\n", "        ]\n", "        \n", "        missing_columns = [col for col in required_columns if col not in df.columns]\n", "        if missing_columns:\n", "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n", "            \n", "        try:\n", "            pd.to_datetime(df['Date'])\n", "        except:\n", "            raise ValueError(\"Date column cannot be parsed as datetime\")\n", "    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n", "        \"\"\"Handle missing values in the DataFrame.\"\"\"\n", "        # First handle boolean columns\n", "        boolean_columns = ['on_off', 'damper']\n", "        df[boolean_columns] = df[boolean_columns].fillna(0)\n", "        \n", "        # Handle numerical columns\n", "        numerical_columns = df.select_dtypes(include=[np.number]).columns\n", "        \n", "        # First try forward fill and backward fill for time series consistency\n", "        df[numerical_columns] = df[numerical_columns].ffill().bfill()\n", "        \n", "        # If any NaNs remain, use KNN imputer\n", "        if df[numerical_columns].isna().any().any():\n", "            df[numerical_columns] = self.imputer.fit_transform(df[numerical_columns])\n", "        \n", "        # Verify no NaNs remain\n", "        if df.isna().any().any():\n", "            raise ValueError(\"Unable to handle all missing values in preprocessing\")\n", "        \n", "        return df\n", "    def _engineer_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n", "        df['datetime'] = pd.to_datetime(df['Date'])\n", "        df['hour'] = df['datetime'].dt.hour\n", "        df['day_of_week'] = df['datetime'].dt.dayofweek\n", "        df['month'] = df['datetime'].dt.month\n", "        df['is_weekend'] = df['datetime'].dt.dayofweek.isin([5, 6]).astype(int)\n", "        df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n", "        df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n", "        df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n", "        df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n", "        df['is_holiday'] = df['datetime'].apply(lambda x: x in self.holidays).astype(int)\n", "        return df\n", "    def _engineer_hvac_features(self, df: pd.DataFrame) -> pd.DataFrame:\n", "        df['temp_difference_in_out'] = df['outlet_temp'] - df['inlet_temp']\n", "        df['temp_difference_ambient'] = df['outside_temp'] - df['inlet_temp']\n", "        df['high_pressure_avg'] = df[['high_pressure_1', 'high_pressure_2', 'high_pressure_3']].mean(axis=1)\n", "        df['low_pressure_avg'] = df[['low_pressure_1', 'low_pressure_2', 'low_pressure_3']].mean(axis=1)\n", "        df['pressure_ratio'] = df['high_pressure_avg'] / (df['low_pressure_avg']+1e-6)\n", "        df['power_per_temp_diff'] = df['active_power'] / (df['temp_difference_in_out'] + 1e-6)\n", "        df['energy_efficiency'] = df['active_energy'] / (df['active_power'] + 1e-6)\n", "        df['temp_setpoint_diff'] = np.where(df['month'].isin([6, 7, 8]),\n", "                                             df['inlet_temp'] - df['summer_setpoint_temp'],\n", "                                             df['inlet_temp'] - df['winter_setpoint_temp'])\n", "        return df\n", "    def _create_rolling_features(self, df: pd.DataFrame, windows: List[int] = [3, 6, 12]) -> pd.DataFrame:\n", "        \"\"\"Create rolling features for key metrics\"\"\"\n", "        key_metrics = ['active_power', 'inlet_temp', 'co2_1', 'amb_humid_1']\n", "        \n", "        for window in windows:\n", "            for metric in key_metrics:\n", "                df[f'{metric}_rolling_mean_{window}h'] = (\n", "                    df[metric].rolling(window=window * 12, min_periods=1).mean()\n", "                )\n", "                df[f'{metric}_rolling_std_{window}h'] = (\n", "                    df[metric].rolling(window=window * 12, min_periods=1).std()\n", "                )\n", "        return df\n", "    def _prepare_target_variable(self, df: pd.DataFrame) -> tuple:\n", "        target = df['active_power']\n", "        features = df.drop(['active_power', 'datetime', 'Date'], axis=1)\n", "        return features, target\n", "    def _scale_features(self, df: pd.DataFrame) -> pd.DataFrame:\n", "        numerical_columns = df.select_dtypes(include=[np.number]).columns\n", "        df[numerical_columns] = self.scaler.fit_transform(df[numerical_columns])\n", "        return df\n", "    def preprocess(self, df: pd.DataFrame, training: bool = True) -> tuple:\n", "        self.logger.info(\"Starting preprocessing pipeline...\")\n", "        self._validate_raw_data(df)\n", "        df = self._handle_missing_values(df)\n", "        df = self._engineer_time_features(df)\n", "        df = self._engineer_hvac_features(df)\n", "        df = self._create_rolling_features(df)\n", "        features, target = self._prepare_target_variable(df)\n", "        features = self._scale_features(features)\n", "        self.logger.info(\"Preprocessing pipeline completed successfully.\")\n", "        return features, target\n", "    def get_feature_names(self) -> list:\n", "        if self.feature_names is None:\n", "            self.feature_names = list(self.scaler.feature_names_in_)\n", "        return self.feature_names"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DataValidator:\n", "    @staticmethod\n", "    def check_data_quality(df: pd.DataFrame) -> dict:\n", "        missing = df.isnull().sum().to_dict()\n", "        return {'quality_score': 1.0, 'missing_columns': [k for k, v in missing.items() if v > 0]}  # Fixed items() call"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class AnomalyDetector:\n", "    def __init__(self, input_dim, method='autoencoder'):\n", "        self.input_dim = input_dim\n", "        self.method = method\n", "        self.model = self._build_model()\n", "        self.threshold = None\n", "        self.scaler = StandardScaler()\n", "        self.verbose = True  # Add verbosity flag\n", "        \n", "    def _build_model(self):\n", "        if self.method == 'autoencoder':\n", "            return self._build_basic_autoencoder()\n", "        elif self.method == 'complex_autoencoder':\n", "            return self._build_complex_autoencoder()\n", "        elif self.method == 'isolation_forest':\n", "            return IsolationForest()\n", "        else:\n", "            raise ValueError(\"Unsupported method\")\n", "    def _build_basic_autoencoder(self):\n", "        input_layer = Input(shape=(self.input_dim,))\n", "        encoder = Dense(32, activation=\"relu\")(input_layer)\n", "        encoder = Dense(16, activation=\"relu\")(encoder)\n", "        encoder = Dropout(0.2)(encoder)\n", "        decoder = Dense(32, activation=\"relu\")(encoder)\n", "        decoder = Dense(self.input_dim, activation=\"linear\")(decoder)\n", "        model = Model(inputs=input_layer, outputs=decoder)\n", "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n", "        return model\n", "    def _build_complex_autoencoder(self):\n", "        input_layer = Input(shape=(self.input_dim,))\n", "        encoded = Dense(64, activation='relu')(input_layer)\n", "        encoded = LayerNormalization()(encoded)\n", "        encoded = Dense(32, activation='relu')(encoded)\n", "        encoded = Dropout(0.2)(encoded)\n", "        encoded = Dense(16, activation='relu')(encoded)\n", "        encoded = LayerNormalization()(encoded)\n", "        decoded = Dense(32, activation='relu')(encoded)\n", "        decoded = LayerNormalization()(decoded)\n", "        decoded = Dense(64, activation='relu')(decoded)\n", "        decoded = Dropout(0.2)(decoded)\n", "        decoded = LayerNormalization()(decoded)\n", "        decoded = Dense(self.input_dim, activation='linear')(decoded)\n", "        model = Model(inputs=input_layer, outputs=decoded)\n", "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n", "        return model\n", "    def fit(self, X, epochs=50, batch_size=32):\n", "        X_scaled = self.scaler.fit_transform(X)\n", "        if isinstance(self.model, Model):\n", "            if self.verbose:\n", "                print(\"Training autoencoder for anomaly detection...\")\n", "                \n", "            # Add progress callback\n", "            progbar = tf.keras.callbacks.ProgressBar(epochs)\n", "            \n", "            # Use larger batch size for speed\n", "            batch_size = min(batch_size * 4, len(X))\n", "            \n", "            self.model.fit(\n", "                X_scaled, X_scaled,\n", "                epochs=epochs,\n", "                batch_size=batch_size,\n", "                verbose=1,\n", "                callbacks=[progbar]\n", "            )\n", "        else:\n", "            if self.verbose:\n", "                print(\"Fitting isolation forest...\")\n", "            self.model.fit(X_scaled)\n", "        \n", "    def detect_anomalies(self, X, threshold=None):\n", "        X_scaled = self.scaler.transform(X)\n", "        if isinstance(self.model, Model):\n", "            errors = np.mean(np.square(X_scaled - self.model.predict(X_scaled)), axis=1)\n", "            self.threshold = threshold or np.percentile(errors, 90)  # Adjusted threshold\n", "            return errors > self.threshold\n", "        else:\n", "            return self.model.predict(X_scaled)\n", "    def get_reconstruction(self, X):\n", "        if not isinstance(self.model, Model):\n", "            raise ValueError(\"Reconstruction only available for autoencoder models\")\n", "        X_scaled = self.scaler.transform(X)\n", "        reconstructed = self.model.predict(X_scaled)\n", "        return self.scaler.inverse_transform(reconstructed)\n", "    \n", "class HVACAnalysisPipeline:\n", "    def __init__(self, config: dict):\n", "        self.config = config\n", "        self.setup_directories()\n", "        self.setup_logging()\n", "        \n", "    def setup_directories(self) -> None:\n", "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n", "        self.output_dir = Path(self.config['output_base_dir']) / f\"analysis_{timestamp}\"\n", "        self.subdirs = {\n", "            'models': self.output_dir / 'models',\n", "            'plots': self.output_dir / 'plots',\n", "            'results': self.output_dir / 'results',\n", "            'anomalies': self.output_dir / 'anomalies',\n", "            'shap': self.output_dir / 'shap'\n", "        }\n", "        for dir_path in self.subdirs.values():\n", "            dir_path.mkdir(parents=True, exist_ok=True)\n", "        \n", "    def setup_logging(self) -> None:\n", "        log_file = self.output_dir / 'pipeline.log'\n", "        logging.basicConfig(\n", "            level=logging.INFO,\n", "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # Fixed format string\n", "            handlers=[logging.FileHandler(log_file), logging.StreamHandler()]\n", "        )\n", "        self.logger = logging.getLogger(__name__)\n", "        \n", "    def load_data(self) -> pd.DataFrame:\n", "        self.logger.info(f\"Loading data from {self.config['data_path']}\")\n", "        try:\n", "            return pd.read_csv(self.config['data_path'], parse_dates=['Date'])\n", "        except Exception as e:\n", "            self.logger.error(f\"Error loading data: {e}\")\n", "            raise\n", "        \n", "    def save_results(self, results: dict, filename: str) -> None:\n", "        file_path = self.subdirs['results'] / filename\n", "        def convert(o):\n", "            return o.item() if hasattr(o, 'item') else o\n", "        with open(file_path, 'w') as f:\n", "            json.dump(results, f, default=convert)\n", "    def plot_model_performance(self, results: dict) -> None:\n", "        plt.figure(figsize=(12, 6))\n", "        for model_name, history in results.get('training_history', {}).items():\n", "            plt.plot(history.history.get('loss', []), label=model_name)\n", "        plt.title('Model Training History')\n", "        plt.xlabel('Epoch')\n", "        plt.ylabel('Loss')\n", "        plt.legend()\n", "        plt.savefig(self.subdirs['plots'] / 'training_history.png')\n", "        plt.close()\n", "        plt.figure(figsize=(10, 6))\n", "        metrics = pd.DataFrame(results.get('evaluation_results', {})).T\n", "        if 'r2' in metrics.columns:\n", "            metrics['r2'].plot(kind='bar')\n", "        plt.title('Model R\u00c2\u00b2 Scores')\n", "        plt.tight_layout()\n", "        plt.savefig(self.subdirs['plots'] / 'model_comparison.png')\n", "        plt.close()\n", "        \n", "    def run_pipeline(self) -> dict:\n", "        try:\n", "            # Load and preprocess data\n", "            df = self.load_data()\n", "            preprocessor = HVACDataPreprocessor(scaler_type='standard', imputer_n_neighbors=5)\n", "            features, target = preprocessor.preprocess(df, training=True)\n", "            \n", "            # Prepare sequences for training\n", "            sequence_length = self.config['model_params']['sequence_length']\n", "            forecast_horizon = self.config['model_params']['forecast_horizon']\n", "            X_sequences, y_sequences = preprocess_data(\n", "                features.values,  # Convert DataFrame to numpy array\n", "                target.values,    # Convert Series to numpy array\n", "                sequence_length=sequence_length,\n", "                forecast_horizon=forecast_horizon\n", "            )\n", "            \n", "            # Split data into train/test sets\n", "            split_idx = int(len(X_sequences) * 0.8)\n", "            X_train = X_sequences[:split_idx]\n", "            X_test = X_sequences[split_idx:]\n", "            y_train = y_sequences[:split_idx]\n", "            y_test = y_sequences[:split_idx:]\n", "            \n", "            # Initialize and train models\n", "            trainer = ModelTrainer(\n", "                input_shape=(sequence_length, features.shape[1]),\n", "                output_dim=forecast_horizon,\n", "                model_dir=self.subdirs['models']\n", "            )\n", "            trainer.load_existing_models()\n", "            trainer.compile_models()\n", "            training_results = trainer.train_all_models(X_train, y_train)\n", "            \n", "            # Evaluate models\n", "            evaluator = ModelEvaluator(preprocessor.scaler)\n", "            evaluation_results = {}\n", "            for name, model in trainer.models.items():\n", "                metrics = evaluator.evaluate_model(model, X_test, y_test)\n", "                evaluation_results[name] = metrics\n", "                self.logger.info(f\"Model {name} evaluation results: {metrics}\")\n", "                \n", "            results = {\n", "                'training_history': training_results,\n", "                'evaluation_results': evaluation_results\n", "            }\n", "            # Save each trained model\n", "            for name, model in trainer.models.items():\n", "                model_path = self.subdirs['models'] / f\"{name}.keras\"\n", "                if not model_path.exists():\n", "                    model.save(model_path)\n", "                    logging.info(f\"Saved model {name} to {model_path}\")\n", "            # Run anomaly detection using autoencoder with progress tracking\n", "            print(\"\\nStarting anomaly detection...\")\n", "            autoencoder = ModelBuilder.build_autoencoder(features.shape[1])\n", "            X_train_flat = X_train.reshape(-1, features.shape[1])\n", "            try:\n", "                # Use multiprocessing for faster data preparation\n", "                X_train_chunks = np.array_split(X_train_flat, psutil.cpu_count())\n", "                with Pool() as pool:\n", "                    X_train_flat = np.vstack(list(tqdm(pool.imap(lambda x: x.reshape(-1), X_train_chunks), total=len(X_train_chunks), desc=\"Preparing data chunks\")))\n", "            except Exception as e:\n", "                logging.warning(f\"Parallel processing failed, falling back to single process: {e}\")\n", "                # Fallback to non-parallel processing\n", "                X_train_flat = X_train.reshape(-1, features.shape[1])\n\n", "            # Fit autoencoder with progress\n", "            autoencoder.fit(X_train_flat, epochs=25, batch_size=64)  # Reduced epochs, larger batch\n", "            \n", "            print(\"\\nRunning SHAP analysis...\")\n", "            # Use smaller subset for SHAP analysis\n", "            sample_size = min(1000, len(X_test))\n", "            X_test_sample = X_test[:sample_size]\n", "            \n", "            best_model_name = max(evaluation_results, key=lambda k: evaluation_results[k].get('r2', 0))\n", "            explainer = ExplainableAI(output_dir=str(self.subdirs['shap']))\n", "            best_model = trainer.models[best_model_name]\n", "            explainer.create_explainer(best_model, X_train[:1000], model_type='lstm')  # Use smaller training set\n", "            shap_values = explainer.compute_shap_values(X_test_sample, model_type='lstm')\n", "            \n", "            print(\"\\nGenerating visualizations...\")\n", "            explainer.plot_shap_summary('lstm', preprocessor.get_feature_names(), X_test_sample)\n", "            \n", "            return results\n", "        except KeyboardInterrupt:\n", "            self.logger.error(\"Pipeline interrupted by user.\")\n", "            raise\n", "        except Exception as e:\n", "            self.logger.error(f\"Pipeline error: {e}\")\n", "            raise\n", "    def generate_recommendations(self, results: dict) -> dict:\n", "        recommendations = {\n", "            'model_selection': {},\n", "            'anomaly_detection': {},\n", "            'system_optimization': {}\n", "        }\n", "        best_model = max(results.get('evaluation_results', {}), key=lambda k: results['evaluation_results'][k].get('r2', 0))\n", "        recommendations['model_selection'] = {\n", "            'best_model': best_model,\n", "            'performance_metrics': results['evaluation_results'][best_model],\n", "            'reason': f\"Selected based on highest R\u00c2\u00b2 score of {results['evaluation_results'][best_model]['r2']:.3f}\"\n", "        }\n", "        return recommendations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def preprocess_data(X, y, sequence_length=24, forecast_horizon=12):\n", "    \"\"\"\n", "    Preprocess time series data into sequences for training.\n", "    \n", "    Args:\n", "        X: Input features array (2D: samples, features)\n", "        y: Target values array (1D)\n", "        sequence_length: Length of input sequences\n", "        forecast_horizon: Number of future steps to predict\n", "    \n", "    Returns:\n", "        Tuple of (X sequences, y sequences)\n", "    \"\"\"\n", "    # Ensure y is 1D\n", "    if isinstance(y, pd.Series):\n", "        y = y.values\n", "    if len(y.shape) > 1:\n", "        y = y.ravel()\n", "    \n", "    # Handle NaN values in input data\n", "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n", "        logging.warning(f\"Found {np.sum(np.isnan(X))} NaN values in features and {np.sum(np.isnan(y))} in target\")\n", "        # Fill NaN values with forward fill, then backward fill\n", "        X = pd.DataFrame(X).ffill().bfill().values\n", "        y = pd.Series(y).ffill().bfill().values\n", "    \n", "    # Calculate valid number of sequences\n", "    num_samples = len(y) - sequence_length - forecast_horizon + 1\n", "    if num_samples <= 0:\n", "        raise ValueError(\"Not enough samples to create sequences\")\n", "    \n", "    # Initialize arrays with correct shapes\n", "    num_features = X.shape[-1]\n", "    X_sequences = np.zeros((num_samples, sequence_length, num_features))\n", "    y_sequences = np.zeros((num_samples, forecast_horizon))\n", "    \n", "    # Create sequences without reshaping input\n", "    for i in range(num_samples):\n", "        X_sequences[i] = X[i:i + sequence_length]\n", "        y_sequences[i] = y[i + sequence_length:i + sequence_length + forecast_horizon]\n", "    \n", "    return X_sequences, y_sequences"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def process_hvac_data(data_path: str) -> Tuple[np.ndarray, np.ndarray]:\n", "    \"\"\"\n", "    Process HVAC data from raw file to model-ready format.\n", "    \n", "    Args:\n", "        data_path: Path to raw data file\n", "    \n", "    Returns:\n", "        Tuple of (processed features, target variable)\n", "    \"\"\"\n", "    # Read data\n", "    df = pd.read_csv(data_path, parse_dates=['Date'])\n", "    \n", "    # Initialize preprocessor\n", "    preprocessor = HVACDataPreprocessor(\n", "        scaler_type='standard',\n", "        imputer_n_neighbors=5\n", "    )\n", "    \n", "    # Validate data quality\n", "    quality_report = DataValidator.check_data_quality(df)\n", "    logging.info(f\"Data quality report: {quality_report}\")\n", "    \n", "    # Preprocess data\n", "    features, target = preprocessor.preprocess(df, training=True)\n", "    \n", "    # Convert target to 1D array if needed\n", "    if isinstance(target, pd.Series):\n", "        target = target.values\n", "    if len(target.shape) > 1:\n", "        target = target.ravel()\n", "    \n", "    # Convert features to numpy array if it's a DataFrame\n", "    if isinstance(features, pd.DataFrame):\n", "        features = features.values\n", "    \n", "    return features, target"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_and_evaluate_models(X_train, X_test, y_train, y_test, features, feature_scaler, target_scaler):\n", "    # Initialize evaluator\n", "    evaluator = ModelEvaluator(feature_scaler, target_scaler)\n", "    \n", "    # Train models\n", "    trainer = ModelTrainer((1, len(features)))\n", "    trainer.compile_models()\n", "    training_results = trainer.train_all_models(X_train, y_train)\n", "    \n", "    # Evaluate models\n", "    evaluation_results = {}\n", "    for name, model in trainer.models.items():\n", "        metrics = evaluator.evaluate_model(model, X_test, y_test)\n", "        evaluation_results[name] = metrics\n", "    \n", "    # Find optimal settings for best model\n", "    best_model_name = max(evaluation_results, key=lambda k: evaluation_results[k]['r2'])\n", "    best_model = trainer.models[best_model_name]\n", "    \n", "    # Build autoencoder for anomaly detection\n", "    autoencoder = ModelBuilder.build_autoencoder(len(features))\n", "    autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, verbose=0)\n", "    \n", "    # Get optimal settings\n", "    optimization_results = {}  # placeholder\n", "    \n", "    return {\n", "        'models': trainer.models,\n", "        'evaluation_results': evaluation_results,\n", "        'optimization_results': optimization_results,\n", "        'best_model_name': best_model_name\n", "    }"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}