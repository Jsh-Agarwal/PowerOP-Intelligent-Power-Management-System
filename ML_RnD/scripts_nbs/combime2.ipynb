{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Any, Tuple, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, Flatten, Bidirectional, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import holidays\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all classes and functions from the individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainableAI:\n",
    "    def __init__(self, output_dir: str):\n",
    "        self.output_dir = output_dir\n",
    "        self.explainers = {}\n",
    "        self.shap_values = {}\n",
    "    def _reshape_input_data(self, data: np.ndarray, model_type: str) -> np.ndarray:\n",
    "        if model_type == 'lstm':\n",
    "            return data.reshape((data.shape[0], 1, -1))\n",
    "        return data\n",
    "    def _create_model_wrapper(self, model, model_type: str):\n",
    "        def predict_wrapper(data):\n",
    "            reshaped_data = self._reshape_input_data(data, model_type)\n",
    "            return model.predict(reshaped_data)\n",
    "        return predict_wrapper\n",
    "    def create_explainer(self, model, X_train: np.ndarray, model_type: str, n_samples: int = 50) -> None:\n",
    "        X_background = shap.sample(X_train, n_samples)\n",
    "        predict_fn = self._create_model_wrapper(model, model_type)\n",
    "        \n",
    "        if model_type in ['lstm', 'autoencoder']:\n",
    "            self.explainers[model_type] = shap.DeepExplainer(predict_fn, X_background)\n",
    "        else:\n",
    "            self.explainers[model_type] = shap.KernelExplainer(predict_fn, X_background)\n",
    "    def compute_shap_values(self, X_test: np.ndarray, model_type: str, n_samples: int = 50) -> np.ndarray:\n",
    "        if model_type not in self.explainers:\n",
    "            raise ValueError(f\"No explainer found for {model_type}\")\n",
    "        X_sample = shap.sample(X_test, n_samples)\n",
    "        self.shap_values[model_type] = self.explainers[model_type].shap_values(X_sample)\n",
    "        return self.shap_values[model_type]\n",
    "    def generate_feature_importance(self, model_type: str, features: list) -> pd.DataFrame:\n",
    "        if model_type not in self.shap_values:\n",
    "            raise ValueError(f\"No shap values computed for {model_type}\")\n",
    "        shap_vals = self.shap_values[model_type]\n",
    "        importance = np.abs(shap_vals).mean(0)\n",
    "        return pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'importance': importance,\n",
    "            'abs_importance': np.abs(importance)\n",
    "        }).sort_values('abs_importance', ascending=False)\n",
    "    def plot_shap_summary(self, model_type: str, features: list, X_test: np.ndarray) -> None:\n",
    "        try:\n",
    "            shap.summary_plot(self.shap_values.get(model_type), X_test, feature_names=features, show=False)\n",
    "            plt.savefig(f\"{self.output_dir}/shap_summary_{model_type}.png\")\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error plotting shap summary: {e}\")\n",
    "    def plot_feature_dependence(self, model_type: str, features: list, X_test: np.ndarray) -> None:\n",
    "        try:\n",
    "            # Plot dependence for first feature as an example.\n",
    "            shap.dependence_plot(features[0], self.shap_values.get(model_type), X_test, show=False)\n",
    "            plt.savefig(f\"{self.output_dir}/shap_dependence_{features[0]}.png\")\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error plotting feature dependence: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilder:\n",
    "    @staticmethod\n",
    "    def build_basic_lstm(input_shape, output_dim=12):\n",
    "        # Minimal implementation\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(64, input_shape=input_shape))\n",
    "        model.add(Dense(output_dim))\n",
    "        model.compile(optimizer=Adam(), loss='mse')\n",
    "        return model\n",
    "    @staticmethod\n",
    "    def build_advanced_lstm(input_shape, output_dim=12):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, return_sequences=True, input_shape=input_shape))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(64))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(output_dim))\n",
    "        model.compile(optimizer=Adam(), loss='mse')\n",
    "        return model\n",
    "    @staticmethod\n",
    "    def build_bi_lstm(input_shape, output_dim=12):\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape))\n",
    "        model.add(Bidirectional(LSTM(64)))\n",
    "        model.add(Dense(output_dim))\n",
    "        model.compile(optimizer=Adam(), loss='mse')\n",
    "        return model\n",
    "    @staticmethod\n",
    "    def build_attention(input_shape, output_dim=12):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        lstm_out = LSTM(128, return_sequences=True)(inputs)\n",
    "        attention_out = MultiHeadAttention(num_heads=2, key_dim=2)(lstm_out, lstm_out)\n",
    "        lstm_out = LSTM(64)(attention_out)\n",
    "        output = Dense(output_dim)(lstm_out)\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "        model.compile(optimizer=Adam(), loss='mse')\n",
    "        return model\n",
    "    @staticmethod\n",
    "    def build_combined_lstm_attention(input_shape, output_dim=12):\n",
    "        return ModelBuilder.build_attention(input_shape, output_dim)\n",
    "    @staticmethod\n",
    "    def build_model(input_shape):\n",
    "        return ModelBuilder.build_basic_lstm(input_shape)\n",
    "    @staticmethod\n",
    "    def build_autoencoder(input_dim, encoding_dim=16):\n",
    "        inputs = Input(shape=(input_dim,))\n",
    "        encoded = Dense(encoding_dim, activation='relu')(inputs)\n",
    "        decoded = Dense(input_dim, activation='linear')(encoded)\n",
    "        autoencoder = Model(inputs=inputs, outputs=decoded)\n",
    "        autoencoder.compile(optimizer=Adam(), loss='mse')\n",
    "        return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, input_shape, output_dim=12, sequence_length=24, model_dir=None):\n",
    "        if len(input_shape) == 2:\n",
    "            self.input_shape = (input_shape[0], input_shape[1])\n",
    "        elif len(input_shape) == 3:\n",
    "            self.input_shape = (input_shape[1], input_shape[2])\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input shape: {input_shape}\")\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.models = {\n",
    "            'bi_lstm': ModelBuilder.build_bi_lstm(self.input_shape, self.output_dim),\n",
    "            'attention': ModelBuilder.build_attention(self.input_shape, self.output_dim),\n",
    "            'combined': ModelBuilder.build_combined_lstm_attention(self.input_shape, self.output_dim)\n",
    "        }\n",
    "        self.sequence_length = sequence_length\n",
    "        self.reshape_required = True\n",
    "        self.model_dir = model_dir or Path(\"D:/PowerAmp/models\")  # Common directory for models\n",
    "        self.pretrained_model_dir = Path(\"D:/PowerAmp/outputs/analysis_20250215_175946/models\")  # Pretrained models directory\n",
    "    def compile_models(self, learning_rate=0.0005):  # Lower learning rate\n",
    "        for name, model in self.models.items():\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=learning_rate),\n",
    "                loss='mse',\n",
    "                metrics=['mae']  # Changed metric to MAE\n",
    "            )\n",
    "    def load_existing_models(self):\n",
    "        if self.pretrained_model_dir:\n",
    "            for name in self.models.keys():\n",
    "                model_path = self.pretrained_model_dir / f\"{name}.keras\"\n",
    "                if model_path.exists():\n",
    "                    self.models[name] = load_model(model_path)\n",
    "                    logging.info(f\"Loaded existing model {name} from {model_path}\")\n",
    "    def _prepare_input_data(self, X):\n",
    "        if self.reshape_required and len(X.shape) == 2:\n",
    "            return X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "        return X\n",
    "    def save_training_history(self, history, model_name):\n",
    "        history_df = pd.DataFrame(history.history)\n",
    "        history_df.to_csv(f\"{self.model_dir}/{model_name}_history.csv\", index=False)\n",
    "    def train_model(self, model, X_train, y_train, epochs=60, batch_size=32, **kwargs):  # Adjusted epochs and batch size\n",
    "        X_train_prepared = self._prepare_input_data(np.asarray(X_train, dtype='float32'))\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),  # Adjusted patience\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8)\n",
    "        ]\n",
    "        history = model.fit(\n",
    "            X_train_prepared, np.asarray(y_train, dtype='float32'),\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            **kwargs\n",
    "        )\n",
    "        logging.info(\"Validation MAE: %s\", history.history.get('val_mae', ['N/A'])[-1])\n",
    "        self.save_training_history(history, model.name)\n",
    "        return history\n",
    "    def train_all_models(self, X_train, y_train, epochs=50, batch_size=32):  # Adjusted epochs and batch size\n",
    "        results = {}\n",
    "        try:\n",
    "            for name, model in self.models.items():\n",
    "                model_path = self.pretrained_model_dir / f\"{name}.keras\"\n",
    "                if model_path.exists():\n",
    "                    logging.info(f\"Loading existing model: {name}\")\n",
    "                    self.models[name] = load_model(model_path)\n",
    "                else:\n",
    "                    logging.info(f\"Training model: {name}\")\n",
    "                    self.train_model(model, X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "                    save_path = self.model_dir / f\"{name}.keras\"\n",
    "                    if not save_path.exists():\n",
    "                        model.save(save_path)\n",
    "                        logging.info(f\"Saved model {name} to {save_path}\")\n",
    "                results[name] = {\"trained\": True}\n",
    "        except KeyboardInterrupt:\n",
    "            logging.info(\"Training interrupted. Saving progress...\")\n",
    "            for name, model in self.models.items():\n",
    "                save_path = self.model_dir / f\"{name}.keras\"\n",
    "                if not save_path.exists():\n",
    "                    model.save(save_path)\n",
    "                    logging.info(f\"Saved model {name} to {save_path}\")\n",
    "            raise\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, feature_scaler=None, target_scaler=None):\n",
    "        self.feature_scaler = feature_scaler\n",
    "        self.target_scaler = target_scaler\n",
    "    @staticmethod\n",
    "    def evaluate_model(model, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate model performance ensuring correct data shapes and handling NaN values.\n",
    "        \"\"\"\n",
    "        # Get predictions - expect X_test to be (samples, timesteps, features)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Reshape predictions and y_test to 1D arrays\n",
    "        predictions = predictions.reshape(-1)\n",
    "        y_test_flat = y_test.reshape(-1)[:predictions.shape[0]]\n",
    "        \n",
    "        # Remove NaN values from both arrays\n",
    "        mask = ~(np.isnan(predictions) | np.isnan(y_test_flat))\n",
    "        predictions_clean = predictions[mask]\n",
    "        y_test_clean = y_test_flat[mask]\n",
    "        \n",
    "        if len(predictions_clean) == 0:\n",
    "            logging.warning(\"No valid predictions after removing NaN values\")\n",
    "            return {\n",
    "                'mae': np.nan,\n",
    "                'mse': np.nan,\n",
    "                'rmse': np.nan,\n",
    "                'r2': np.nan,\n",
    "                'test_loss': np.nan,\n",
    "                'test_acc': np.nan,\n",
    "                'n_samples': 0,\n",
    "                'n_nans': np.sum(~mask)\n",
    "            }\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test_clean, predictions_clean)\n",
    "        mse = mean_squared_error(y_test_clean, predictions_clean)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test_clean, predictions_clean)\n",
    "        \n",
    "        # Model evaluation with cleaned data\n",
    "        try:\n",
    "            loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "        except ValueError:\n",
    "            loss, test_acc = -1, -1\n",
    "            logging.warning(\"Could not calculate test accuracy due to shape mismatch\")\n",
    "        \n",
    "        return {\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'test_loss': loss,\n",
    "            'test_acc': test_acc,\n",
    "            'n_samples': len(predictions_clean),\n",
    "            'n_nans': np.sum(~mask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HVACDataPreprocessor:\n",
    "    def __init__(self, scaler_type: str = 'standard', imputer_n_neighbors: int = 5, country_holidays: str = 'US'):\n",
    "        self.scaler_type = scaler_type\n",
    "        self.scaler = StandardScaler() if scaler_type == 'standard' else MinMaxScaler()\n",
    "        self.imputer = KNNImputer(n_neighbors=imputer_n_neighbors)\n",
    "        self.holidays = holidays.CountryHoliday(country_holidays)\n",
    "        self.feature_names = None\n",
    "        self.numerical_columns = None\n",
    "        self._setup_logging()\n",
    "        \n",
    "    def _setup_logging(self):\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'  # Fixed format string\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    def _validate_raw_data(self, df: pd.DataFrame) -> None:\n",
    "        required_columns = [\n",
    "            'Date', 'on_off', 'damper', 'active_energy', 'co2_1', 'amb_humid_1',\n",
    "            'active_power', 'pot_gen', 'high_pressure_1', 'high_pressure_2',\n",
    "            'low_pressure_1', 'low_pressure_2', 'high_pressure_3', 'low_pressure_3',\n",
    "            'outside_temp', 'outlet_temp', 'inlet_temp', 'summer_setpoint_temp',\n",
    "            'winter_setpoint_temp', 'amb_temp_2'\n",
    "        ]\n",
    "        \n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "            \n",
    "        try:\n",
    "            pd.to_datetime(df['Date'])\n",
    "        except:\n",
    "            raise ValueError(\"Date column cannot be parsed as datetime\")\n",
    "    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handle missing values in the DataFrame.\"\"\"\n",
    "        # First handle boolean columns\n",
    "        boolean_columns = ['on_off', 'damper']\n",
    "        df[boolean_columns] = df[boolean_columns].fillna(0)\n",
    "        \n",
    "        # Handle numerical columns\n",
    "        numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        # First try forward fill and backward fill for time series consistency\n",
    "        df[numerical_columns] = df[numerical_columns].ffill().bfill()\n",
    "        \n",
    "        # If any NaNs remain, use KNN imputer\n",
    "        if df[numerical_columns].isna().any().any():\n",
    "            df[numerical_columns] = self.imputer.fit_transform(df[numerical_columns])\n",
    "        \n",
    "        # Verify no NaNs remain\n",
    "        if df.isna().any().any():\n",
    "            raise ValueError(\"Unable to handle all missing values in preprocessing\")\n",
    "        \n",
    "        return df\n",
    "    def _engineer_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df['datetime'] = pd.to_datetime(df['Date'])\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['is_weekend'] = df['datetime'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "        df['is_holiday'] = df['datetime'].apply(lambda x: x in self.holidays).astype(int)\n",
    "        return df\n",
    "    def _engineer_hvac_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df['temp_difference_in_out'] = df['outlet_temp'] - df['inlet_temp']\n",
    "        df['temp_difference_ambient'] = df['outside_temp'] - df['inlet_temp']\n",
    "        df['high_pressure_avg'] = df[['high_pressure_1', 'high_pressure_2', 'high_pressure_3']].mean(axis=1)\n",
    "        df['low_pressure_avg'] = df[['low_pressure_1', 'low_pressure_2', 'low_pressure_3']].mean(axis=1)\n",
    "        df['pressure_ratio'] = df['high_pressure_avg'] / (df['low_pressure_avg']+1e-6)\n",
    "        df['power_per_temp_diff'] = df['active_power'] / (df['temp_difference_in_out'] + 1e-6)\n",
    "        df['energy_efficiency'] = df['active_energy'] / (df['active_power'] + 1e-6)\n",
    "        df['temp_setpoint_diff'] = np.where(df['month'].isin([6, 7, 8]),\n",
    "                                             df['inlet_temp'] - df['summer_setpoint_temp'],\n",
    "                                             df['inlet_temp'] - df['winter_setpoint_temp'])\n",
    "        return df\n",
    "    def _create_rolling_features(self, df: pd.DataFrame, windows: List[int] = [3, 6, 12]) -> pd.DataFrame:\n",
    "        \"\"\"Create rolling features for key metrics\"\"\"\n",
    "        key_metrics = ['active_power', 'inlet_temp', 'co2_1', 'amb_humid_1']\n",
    "        \n",
    "        for window in windows:\n",
    "            for metric in key_metrics:\n",
    "                df[f'{metric}_rolling_mean_{window}h'] = (\n",
    "                    df[metric].rolling(window=window * 12, min_periods=1).mean()\n",
    "                )\n",
    "                df[f'{metric}_rolling_std_{window}h'] = (\n",
    "                    df[metric].rolling(window=window * 12, min_periods=1).std()\n",
    "                )\n",
    "        return df\n",
    "    def _prepare_target_variable(self, df: pd.DataFrame) -> tuple:\n",
    "        target = df['active_power']\n",
    "        features = df.drop(['active_power', 'datetime', 'Date'], axis=1)\n",
    "        return features, target\n",
    "    def _scale_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_columns] = self.scaler.fit_transform(df[numerical_columns])\n",
    "        return df\n",
    "    def preprocess(self, df: pd.DataFrame, training: bool = True) -> tuple:\n",
    "        self.logger.info(\"Starting preprocessing pipeline...\")\n",
    "        self._validate_raw_data(df)\n",
    "        df = self._handle_missing_values(df)\n",
    "        df = self._engineer_time_features(df)\n",
    "        df = self._engineer_hvac_features(df)\n",
    "        df = self._create_rolling_features(df)\n",
    "        features, target = self._prepare_target_variable(df)\n",
    "        features = self._scale_features(features)\n",
    "        self.logger.info(\"Preprocessing pipeline completed successfully.\")\n",
    "        return features, target\n",
    "    def get_feature_names(self) -> list:\n",
    "        if self.feature_names is None:\n",
    "            self.feature_names = list(self.scaler.feature_names_in_)\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    @staticmethod\n",
    "    def check_data_quality(df: pd.DataFrame) -> dict:\n",
    "        missing = df.isnull().sum().to_dict()\n",
    "        return {'quality_score': 1.0, 'missing_columns': [k for k, v in missing.items() if v > 0]}  # Fixed items() call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector:\n",
    "    def __init__(self, input_dim, method='autoencoder'):\n",
    "        self.input_dim = input_dim\n",
    "        self.method = method\n",
    "        self.model = self._build_model()\n",
    "        self.threshold = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        if self.method == 'autoencoder':\n",
    "            return self._build_basic_autoencoder()\n",
    "        elif self.method == 'complex_autoencoder':\n",
    "            return self._build_complex_autoencoder()\n",
    "        elif self.method == 'isolation_forest':\n",
    "            return IsolationForest()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported method\")\n",
    "    def _build_basic_autoencoder(self):\n",
    "        input_layer = Input(shape=(self.input_dim,))\n",
    "        encoder = Dense(32, activation=\"relu\")(input_layer)\n",
    "        encoder = Dense(16, activation=\"relu\")(encoder)\n",
    "        encoder = Dropout(0.2)(encoder)\n",
    "        decoder = Dense(32, activation=\"relu\")(encoder)\n",
    "        decoder = Dense(self.input_dim, activation=\"linear\")(decoder)\n",
    "        model = Model(inputs=input_layer, outputs=decoder)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    def _build_complex_autoencoder(self):\n",
    "        input_layer = Input(shape=(self.input_dim,))\n",
    "        encoded = Dense(64, activation='relu')(input_layer)\n",
    "        encoded = LayerNormalization()(encoded)\n",
    "        encoded = Dense(32, activation='relu')(encoded)\n",
    "        encoded = Dropout(0.2)(encoded)\n",
    "        encoded = Dense(16, activation='relu')(encoded)\n",
    "        encoded = LayerNormalization()(encoded)\n",
    "        decoded = Dense(32, activation='relu')(encoded)\n",
    "        decoded = LayerNormalization()(decoded)\n",
    "        decoded = Dense(64, activation='relu')(decoded)\n",
    "        decoded = Dropout(0.2)(decoded)\n",
    "        decoded = LayerNormalization()(decoded)\n",
    "        decoded = Dense(self.input_dim, activation='linear')(decoded)\n",
    "        model = Model(inputs=input_layer, outputs=decoded)\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    def fit(self, X, epochs=50, batch_size=32):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        if isinstance(self.model, Model):\n",
    "            self.model.fit(X_scaled, X_scaled, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        else:\n",
    "            self.model.fit(X_scaled)\n",
    "        \n",
    "    def detect_anomalies(self, X, threshold=None):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        if isinstance(self.model, Model):\n",
    "            errors = np.mean(np.square(X_scaled - self.model.predict(X_scaled)), axis=1)\n",
    "            self.threshold = threshold or np.percentile(errors, 90)  # Adjusted threshold\n",
    "            return errors > self.threshold\n",
    "        else:\n",
    "            return self.model.predict(X_scaled)\n",
    "    def get_reconstruction(self, X):\n",
    "        if not isinstance(self.model, Model):\n",
    "            raise ValueError(\"Reconstruction only available for autoencoder models\")\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        reconstructed = self.model.predict(X_scaled)\n",
    "        return self.scaler.inverse_transform(reconstructed)\n",
    "    \n",
    "class HVACAnalysisPipeline:\n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        self.setup_directories()\n",
    "        self.setup_logging()\n",
    "        \n",
    "    def setup_directories(self) -> None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.output_dir = Path(self.config['output_base_dir']) / f\"analysis_{timestamp}\"\n",
    "        self.subdirs = {\n",
    "            'models': self.output_dir / 'models',\n",
    "            'plots': self.output_dir / 'plots',\n",
    "            'results': self.output_dir / 'results',\n",
    "            'anomalies': self.output_dir / 'anomalies',\n",
    "            'shap': self.output_dir / 'shap'\n",
    "        }\n",
    "        for dir_path in self.subdirs.values():\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def setup_logging(self) -> None:\n",
    "        log_file = self.output_dir / 'pipeline.log'\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # Fixed format string\n",
    "            handlers=[logging.FileHandler(log_file), logging.StreamHandler()]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        self.logger.info(f\"Loading data from {self.config['data_path']}\")\n",
    "        try:\n",
    "            return pd.read_csv(self.config['data_path'], parse_dates=['Date'])\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "        \n",
    "    def save_results(self, results: dict, filename: str) -> None:\n",
    "        file_path = self.subdirs['results'] / filename\n",
    "        def convert(o):\n",
    "            return o.item() if hasattr(o, 'item') else o\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(results, f, default=convert)\n",
    "    def plot_model_performance(self, results: dict) -> None:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for model_name, history in results.get('training_history', {}).items():\n",
    "            plt.plot(history.history.get('loss', []), label=model_name)\n",
    "        plt.title('Model Training History')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(self.subdirs['plots'] / 'training_history.png')\n",
    "        plt.close()\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        metrics = pd.DataFrame(results.get('evaluation_results', {})).T\n",
    "        if 'r2' in metrics.columns:\n",
    "            metrics['r2'].plot(kind='bar')\n",
    "        plt.title('Model RÂ² Scores')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.subdirs['plots'] / 'model_comparison.png')\n",
    "        plt.close()\n",
    "        \n",
    "    def run_pipeline(self) -> dict:\n",
    "        try:\n",
    "            # Load and preprocess data\n",
    "            df = self.load_data()\n",
    "            preprocessor = HVACDataPreprocessor(scaler_type='standard', imputer_n_neighbors=5)\n",
    "            features, target = preprocessor.preprocess(df, training=True)\n",
    "            \n",
    "            # Prepare sequences for training\n",
    "            sequence_length = self.config['model_params']['sequence_length']\n",
    "            forecast_horizon = self.config['model_params']['forecast_horizon']\n",
    "            X_sequences, y_sequences = preprocess_data(\n",
    "                features.values,  # Convert DataFrame to numpy array\n",
    "                target.values,    # Convert Series to numpy array\n",
    "                sequence_length=sequence_length,\n",
    "                forecast_horizon=forecast_horizon\n",
    "            )\n",
    "            \n",
    "            # Split data into train/test sets\n",
    "            split_idx = int(len(X_sequences) * 0.8)\n",
    "            X_train = X_sequences[:split_idx]\n",
    "            X_test = X_sequences[split_idx:]\n",
    "            y_train = y_sequences[:split_idx]\n",
    "            y_test = y_sequences[:split_idx:]\n",
    "            \n",
    "            # Initialize and train models\n",
    "            trainer = ModelTrainer(\n",
    "                input_shape=(sequence_length, features.shape[1]),\n",
    "                output_dim=forecast_horizon,\n",
    "                model_dir=self.subdirs['models']\n",
    "            )\n",
    "            trainer.load_existing_models()\n",
    "            trainer.compile_models()\n",
    "            training_results = trainer.train_all_models(X_train, y_train)\n",
    "            \n",
    "            # Evaluate models\n",
    "            evaluator = ModelEvaluator(preprocessor.scaler)\n",
    "            evaluation_results = {}\n",
    "            for name, model in trainer.models.items():\n",
    "                metrics = evaluator.evaluate_model(model, X_test, y_test)\n",
    "                evaluation_results[name] = metrics\n",
    "                self.logger.info(f\"Model {name} evaluation results: {metrics}\")\n",
    "                \n",
    "            results = {\n",
    "                'training_history': training_results,\n",
    "                'evaluation_results': evaluation_results\n",
    "            }\n",
    "            # Save each trained model\n",
    "            for name, model in trainer.models.items():\n",
    "                model_path = self.subdirs['models'] / f\"{name}.keras\"\n",
    "                model.save(model_path)\n",
    "                logging.info(f\"Saved model {name} to {model_path}\")\n",
    "            # Run anomaly detection using autoencoder\n",
    "            autoencoder = ModelBuilder.build_autoencoder(features.shape[1])\n",
    "            # Prepare training data for autoencoder (flatten sequences)\n",
    "            X_train_flat = X_train.reshape(-1, features.shape[1])\n",
    "            autoencoder.fit(X_train_flat, X_train_flat, epochs=50, batch_size=32, verbose=0)\n",
    "            # Detect anomalies on test data (flatten sequences)\n",
    "            X_test_flat = X_test.reshape(-1, features.shape[1])\n",
    "            reconstructions = autoencoder.predict(X_test_flat)\n",
    "            reconstruction_errors = np.mean(np.square(X_test_flat - reconstructions), axis=1)\n",
    "            anomaly_threshold = np.percentile(reconstruction_errors, 90)\n",
    "            anomalies = (reconstruction_errors > anomaly_threshold).tolist()\n",
    "            results['anomaly_detection'] = {\n",
    "                'threshold': float(anomaly_threshold),\n",
    "                'n_anomalies': int(sum(anomalies)),\n",
    "                'reconstruction_errors': reconstruction_errors.tolist()  # Added reconstruction_errors to results\n",
    "            }\n",
    "            # Run SHAP analysis (for example, on best model using ExplainableAI)\n",
    "            best_model_name = max(evaluation_results, key=lambda k: evaluation_results[k].get('r2', 0))\n",
    "            explainer = ExplainableAI(output_dir=str(self.subdirs['shap']))\n",
    "            best_model = trainer.models[best_model_name]\n",
    "            # Use a sample from X_train_flat for background; adjust reshape as needed\n",
    "            explainer.create_explainer(best_model, X_train_flat, model_type='lstm')\n",
    "            shap_values = explainer.compute_shap_values(X_test_flat, model_type='lstm')\n",
    "            explainer.plot_shap_summary('lstm', preprocessor.get_feature_names(), X_test_flat)\n",
    "            results['shap'] = {'model': best_model_name, 'shap_summary': f\"shap_summary_lstm.png\"}\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline error: {e}\")\n",
    "            raise\n",
    "    def generate_recommendations(self, results: dict) -> dict:\n",
    "        recommendations = {\n",
    "            'model_selection': {},\n",
    "            'anomaly_detection': {},\n",
    "            'system_optimization': {}\n",
    "        }\n",
    "        best_model = max(results.get('evaluation_results', {}), key=lambda k: results['evaluation_results'][k].get('r2', 0))\n",
    "        recommendations['model_selection'] = {\n",
    "            'best_model': best_model,\n",
    "            'performance_metrics': results['evaluation_results'][best_model],\n",
    "            'reason': f\"Selected based on highest RÂ² score of {results['evaluation_results'][best_model]['r2']:.3f}\"\n",
    "        }\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, y, sequence_length=24, forecast_horizon=12):\n",
    "    \"\"\"\n",
    "    Preprocess time series data into sequences for training.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features array (2D: samples, features)\n",
    "        y: Target values array (1D)\n",
    "        sequence_length: Length of input sequences\n",
    "        forecast_horizon: Number of future steps to predict\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X sequences, y sequences)\n",
    "    \"\"\"\n",
    "    # Ensure y is 1D\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.values\n",
    "    if len(y.shape) > 1:\n",
    "        y = y.ravel()\n",
    "    \n",
    "    # Handle NaN values in input data\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        logging.warning(f\"Found {np.sum(np.isnan(X))} NaN values in features and {np.sum(np.isnan(y))} in target\")\n",
    "        # Fill NaN values with forward fill, then backward fill\n",
    "        X = pd.DataFrame(X).ffill().bfill().values\n",
    "        y = pd.Series(y).ffill().bfill().values\n",
    "    \n",
    "    # Calculate valid number of sequences\n",
    "    num_samples = len(y) - sequence_length - forecast_horizon + 1\n",
    "    \n",
    "    if num_samples <= 0:\n",
    "        raise ValueError(\"Not enough samples to create sequences\")\n",
    "    \n",
    "    # Initialize arrays with correct shapes\n",
    "    num_features = X.shape[-1]\n",
    "    X_sequences = np.zeros((num_samples, sequence_length, num_features))\n",
    "    y_sequences = np.zeros((num_samples, forecast_horizon))\n",
    "    \n",
    "    # Create sequences without reshaping input\n",
    "    for i in range(num_samples):\n",
    "        X_sequences[i] = X[i:i + sequence_length]\n",
    "        y_sequences[i] = y[i + sequence_length:i + sequence_length + forecast_horizon]\n",
    "    \n",
    "    return X_sequences, y_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hvac_data(data_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Process HVAC data from raw file to model-ready format.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to raw data file\n",
    "    Returns:\n",
    "        Tuple of (processed features, target variable)\n",
    "    \"\"\"\n",
    "    # Read data\n",
    "    df = pd.read_csv(data_path, parse_dates=['Date'])\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = HVACDataPreprocessor(\n",
    "        scaler_type='standard',\n",
    "        imputer_n_neighbors=5\n",
    "    )\n",
    "    \n",
    "    # Validate data quality\n",
    "    quality_report = DataValidator.check_data_quality(df)\n",
    "    logging.info(f\"Data quality report: {quality_report}\")\n",
    "    \n",
    "    # Preprocess data\n",
    "    features, target = preprocessor.preprocess(df, training=True)\n",
    "    \n",
    "    # Convert target to 1D array if needed\n",
    "    if isinstance(target, pd.Series):\n",
    "        target = target.values\n",
    "    if len(target.shape) > 1:\n",
    "        target = target.ravel()\n",
    "    \n",
    "    # Convert features to numpy array if it's a DataFrame\n",
    "    if isinstance(features, pd.DataFrame):\n",
    "        features = features.values\n",
    "        \n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test, features, feature_scaler, target_scaler):\n",
    "    # Initialize evaluator\n",
    "    evaluator = ModelEvaluator(feature_scaler, target_scaler)\n",
    "    \n",
    "    # Train models\n",
    "    trainer = ModelTrainer((1, len(features)))\n",
    "    trainer.compile_models()\n",
    "    training_results = trainer.train_all_models(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    evaluation_results = {}\n",
    "    for name, model in trainer.models.items():\n",
    "        metrics = evaluator.evaluate_model(model, X_test, y_test)\n",
    "        evaluation_results[name] = metrics\n",
    "    \n",
    "    # Find optimal settings for best model\n",
    "    best_model_name = max(evaluation_results, key=lambda k: evaluation_results[k]['r2'])\n",
    "    best_model = trainer.models[best_model_name]\n",
    "    \n",
    "    # Build autoencoder for anomaly detection\n",
    "    autoencoder = ModelBuilder.build_autoencoder(len(features))\n",
    "    autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Get optimal settings\n",
    "    optimization_results = {}  # placeholder\n",
    "    \n",
    "    return {\n",
    "        'models': trainer.models,\n",
    "        'evaluation_results': evaluation_results,\n",
    "        'optimization_results': optimization_results,\n",
    "        'best_model_name': best_model_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedHVACPipeline(HVACAnalysisPipeline):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__(config)\n",
    "        self.power_cost_per_kwh = config.get('power_cost_per_kwh', 0.12)  # Default electricity cost\n",
    "        self.setup_ml_models()\n",
    "        \n",
    "    def setup_ml_models(self):\n",
    "        \"\"\"Initialize additional ML models for comparison\"\"\"\n",
    "        self.ml_models = {\n",
    "            'random_forest': RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=20,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=4,\n",
    "                n_jobs=-1,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'gradient_boosting': GradientBoostingRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.1,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=4,\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "    def determine_comfort_range(self, df: pd.DataFrame) -> Tuple[float, float]:\n",
    "        \"\"\"Determine comfort range from the data\"\"\"\n",
    "        winter_temps = df[df['month'].isin([12, 1, 2])]['inlet_temp']\n",
    "        summer_temps = df[df['month'].isin([6, 7, 8])]['inlet_temp']\n",
    "        winter_comfort_range = (winter_temps.quantile(0.25), winter_temps.quantile(0.75))\n",
    "        summer_comfort_range = (summer_temps.quantile(0.25), summer_temps.quantile(0.75))\n",
    "        return winter_comfort_range, summer_comfort_range\n",
    "    def optimize_setpoint(self, features: pd.DataFrame, current_temp: float, comfort_range: Tuple[float, float]) -> dict:\n",
    "        \"\"\"Optimize HVAC setpoint for minimum power consumption\"\"\"\n",
    "        def power_consumption(setpoint):\n",
    "            # Create feature vector with new setpoint\n",
    "            test_features = features.copy()\n",
    "            test_features['temp_setpoint'] = setpoint\n",
    "            \n",
    "            # Predict power consumption\n",
    "            power = self.best_model.predict(test_features.values.reshape(1, -1))[0]\n",
    "            \n",
    "            # Penalty for comfort violation\n",
    "            comfort_penalty = 0\n",
    "            if setpoint < comfort_range[0]:\n",
    "                comfort_penalty = (comfort_range[0] - setpoint) * 1000\n",
    "            elif setpoint > comfort_range[1]:\n",
    "                comfort_penalty = (setpoint - comfort_range[1]) * 1000\n",
    "                \n",
    "            return power + comfort_penalty\n",
    "\n",
    "        # Optimize setpoint\n",
    "        result = minimize(\n",
    "            power_consumption,\n",
    "            x0=current_temp,\n",
    "            bounds=[(comfort_range[0]-2, comfort_range[1]+2)],\n",
    "            method='Powell'\n",
    "        )\n",
    "        \n",
    "        optimal_setpoint = result.x[0]\n",
    "        predicted_power = power_consumption(optimal_setpoint) - comfort_penalty\n",
    "        cost_savings = (power_consumption(current_temp) - predicted_power) * self.power_cost_per_kwh * 24\n",
    "        \n",
    "        return {\n",
    "            'optimal_setpoint': optimal_setpoint,\n",
    "            'predicted_power_savings': power_consumption(current_temp) - predicted_power,\n",
    "            'daily_cost_savings': cost_savings,\n",
    "            'current_power': power_consumption(current_temp)\n",
    "        }\n",
    "    def create_dashboard(self, df: pd.DataFrame, results: dict) -> None:\n",
    "        \"\"\"Create comprehensive dashboard using plotly\"\"\"\n",
    "        # Create dashboard layout\n",
    "        fig = make_subplots(\n",
    "            rows=3, cols=2,\n",
    "            subplot_titles=(\n",
    "                'Power Consumption Pattern',\n",
    "                'Temperature vs Power',\n",
    "                'Model Performance Comparison',\n",
    "                'Anomaly Detection',\n",
    "                'SHAP Feature Importance',\n",
    "                'Cost Analysis'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Power consumption pattern\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['datetime'],\n",
    "                y=df['active_power'],\n",
    "                name='Power Consumption'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Temperature vs Power\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['inlet_temp'],\n",
    "                y=df['active_power'],\n",
    "                mode='markers',\n",
    "                name='Temp vs Power'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Model performance comparison\n",
    "        model_names = list(results['evaluation_results'].keys())\n",
    "        r2_scores = [results['evaluation_results'][model]['r2'] for model in model_names]\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=model_names, y=r2_scores, name='RÂ² Score'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Anomaly detection\n",
    "        anomaly_scores = results['anomaly_detection']['reconstruction_errors']\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['datetime'][-len(anomaly_scores):],\n",
    "                y=anomaly_scores,\n",
    "                name='Anomaly Score'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # SHAP importance\n",
    "        feature_importance = results['shap']['feature_importance']\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=feature_importance['feature'],\n",
    "                y=feature_importance['importance'],\n",
    "                name='SHAP Importance'\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        # Cost analysis\n",
    "        daily_costs = df.groupby(df['datetime'].dt.date)['active_power'].sum() * self.power_cost_per_kwh\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=daily_costs.index,\n",
    "                y=daily_costs.values,\n",
    "                name='Daily Cost'\n",
    "            ),\n",
    "            row=3, col=2\n",
    "        )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(height=1200, width=1600, showlegend=True)\n",
    "        \n",
    "        # Save dashboard\n",
    "        fig.write_html(str(self.subdirs['plots'] / 'dashboard.html'))\n",
    "    def run_enhanced_pipeline(self) -> dict:\n",
    "        \"\"\"Run enhanced pipeline with all new features\"\"\"\n",
    "        try:\n",
    "            # Run basic pipeline first\n",
    "            base_results = self.run_pipeline()\n",
    "            \n",
    "            # Load and preprocess data\n",
    "            df = self.load_data()\n",
    "            preprocessor = HVACDataPreprocessor()\n",
    "            features, target = preprocessor.preprocess(df)\n",
    "            \n",
    "            # Determine comfort range from data\n",
    "            winter_comfort_range, summer_comfort_range = self.determine_comfort_range(df)\n",
    "            \n",
    "            # Train ML models for comparison\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                features, target, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            ml_results = {}\n",
    "            for name, model in self.ml_models.items():\n",
    "                model.fit(X_train, y_train)\n",
    "                ml_results[name] = {\n",
    "                    'r2': r2_score(y_test, model.predict(X_test)),\n",
    "                    'mae': mean_absolute_error(y_test, model.predict(X_test))\n",
    "                }\n",
    "            \n",
    "            # Optimize setpoints\n",
    "            current_conditions = features.iloc[-1]\n",
    "            current_month = current_conditions['month']\n",
    "            comfort_range = summer_comfort_range if current_month in [6, 7, 8] else winter_comfort_range\n",
    "            optimization_results = self.optimize_setpoint(\n",
    "                current_conditions,\n",
    "                current_conditions['inlet_temp'],\n",
    "                comfort_range\n",
    "            )\n",
    "            \n",
    "            # Analyze usage patterns\n",
    "            usage_patterns = self.analyze_usage_patterns(df)\n",
    "            \n",
    "            # Enhanced anomaly detection\n",
    "            anomaly_detector = AnomalyDetector(features.shape[1], method='complex_autoencoder')\n",
    "            anomaly_detector.fit(features.values)\n",
    "            anomalies = anomaly_detector.detect_anomalies(features.values)\n",
    "            \n",
    "            # Create detailed SHAP analysis\n",
    "            explainer = ExplainableAI(str(self.subdirs['shap']))\n",
    "            best_model = self.ml_models[max(ml_results, key=lambda k: ml_results[k]['r2'])]\n",
    "            explainer.create_explainer(best_model, X_train, 'ml')\n",
    "            shap_values = explainer.compute_shap_values(X_test, 'ml')\n",
    "            feature_importance = explainer.generate_feature_importance('ml', features.columns)\n",
    "            \n",
    "            # Combine all results\n",
    "            enhanced_results = {\n",
    "                **base_results,\n",
    "                'ml_results': ml_results,\n",
    "                'optimization_results': optimization_results,\n",
    "                'usage_patterns': usage_patterns,\n",
    "                'anomalies': {\n",
    "                    'indices': np.where(anomalies)[0].tolist(),\n",
    "                    'total_count': sum(anomalies),\n",
    "                    'anomaly_dates': df.iloc[np.where(anomalies)[0]]['datetime'].tolist()\n",
    "                },\n",
    "                'shap': {\n",
    "                    'feature_importance': feature_importance.to_dict(),\n",
    "                    'summary_plot': 'shap_summary_ml.png'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Create dashboard\n",
    "            self.create_dashboard(df, enhanced_results)\n",
    "            \n",
    "            return enhanced_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Enhanced pipeline error: {e}\")\n",
    "            raise\n",
    "    def generate_recommendations(self, results: dict) -> dict:\n",
    "        \"\"\"Generate enhanced recommendations including cost and comfort optimization\"\"\"\n",
    "        recommendations = super().generate_recommendations(results)\n",
    "        \n",
    "        # Add optimization recommendations\n",
    "        recommendations['setpoint_optimization'] = {\n",
    "            'optimal_setpoint': results['optimization_results']['optimal_setpoint'],\n",
    "            'potential_savings': results['optimization_results']['daily_cost_savings'],\n",
    "            'comfort_impact': 'Minimal - Within preferred range'\n",
    "        }\n",
    "        \n",
    "        # Add usage pattern recommendations\n",
    "        recommendations['usage_optimization'] = {\n",
    "            'peak_hours': f\"Consider reducing usage during {results['usage_patterns']['peak_hours']}\",\n",
    "            'off_peak_suggestion': f\"Shift non-essential cooling to {results['usage_patterns']['off_peak_hours']}\"\n",
    "        }\n",
    "        \n",
    "        # Add anomaly-based recommendations\n",
    "        if results['anomalies']['total_count'] > 0:\n",
    "            recommendations['maintenance'] = {\n",
    "                'anomaly_detected': True,\n",
    "                'suggestion': 'Schedule maintenance check - Unusual patterns detected',\n",
    "                'dates_of_concern': results['anomalies']['anomaly_dates'][-5:]  # Last 5 anomalies\n",
    "            }\n",
    "            \n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 20:50:27,452 - __main__ - INFO - Loading data from HVAC_dataset.csv\n",
      "2025-02-15 20:50:31,253 - __main__ - INFO - Starting preprocessing pipeline...\n",
      "2025-02-15 20:50:31,627 - __main__ - INFO - Preprocessing pipeline completed successfully.\n",
      "2025-02-15 20:50:31,636 - root - WARNING - Found 12 NaN values in features and 0 in target\n",
      "2025-02-15 20:50:32,625 - root - INFO - Loaded existing model bi_lstm from D:\\PowerAmp\\outputs\\analysis_20250215_175946\\models\\bi_lstm.keras\n",
      "2025-02-15 20:50:32,951 - root - INFO - Loaded existing model attention from D:\\PowerAmp\\outputs\\analysis_20250215_175946\\models\\attention.keras\n",
      "2025-02-15 20:50:33,273 - root - INFO - Loaded existing model combined from D:\\PowerAmp\\outputs\\analysis_20250215_175946\\models\\combined.keras\n",
      "2025-02-15 20:50:33,291 - root - INFO - Loading existing model: bi_lstm\n",
      "2025-02-15 20:50:33,575 - root - INFO - Loading existing model: attention\n",
      "2025-02-15 20:50:33,849 - root - INFO - Loading existing model: combined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 20:50:38,153 - root - WARNING - Could not calculate test accuracy due to shape mismatch\n",
      "2025-02-15 20:50:38,154 - __main__ - INFO - Model bi_lstm evaluation results: {'mae': 11.698757335373587, 'mse': 322.9415349110574, 'rmse': np.float64(17.970574139716778), 'r2': -0.0006025857620333142, 'test_loss': -1, 'test_acc': -1, 'n_samples': 61440, 'n_nans': np.int64(0)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 20:50:41,680 - root - WARNING - Could not calculate test accuracy due to shape mismatch\n",
      "2025-02-15 20:50:41,681 - __main__ - INFO - Model attention evaluation results: {'mae': 11.255048413495281, 'mse': 314.56921273388764, 'rmse': np.float64(17.736099140845138), 'r2': 0.025338231115597853, 'test_loss': -1, 'test_acc': -1, 'n_samples': 61440, 'n_nans': np.int64(0)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 20:50:45,253 - root - WARNING - Could not calculate test accuracy due to shape mismatch\n",
      "2025-02-15 20:50:45,256 - __main__ - INFO - Model combined evaluation results: {'mae': 11.143199546157478, 'mse': 326.28194772325, 'rmse': np.float64(18.06327621787504), 'r2': -0.010952526342804969, 'test_loss': -1, 'test_acc': -1, 'n_samples': 61440, 'n_nans': np.int64(0)}\n",
      "2025-02-15 20:50:45,418 - root - INFO - Saved model bi_lstm to outputs\\analysis_20250215_205027\\models\\bi_lstm.keras\n",
      "2025-02-15 20:50:45,598 - root - INFO - Saved model attention to outputs\\analysis_20250215_205027\\models\\attention.keras\n",
      "2025-02-15 20:50:45,709 - root - INFO - Saved model combined to outputs\\analysis_20250215_205027\\models\\combined.keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3840/3840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 21:18:32,447 - __main__ - ERROR - Pipeline error: module 'shap' has no attribute 'sample'\n",
      "2025-02-15 21:18:32,449 - __main__ - ERROR - Enhanced pipeline error: module 'shap' has no attribute 'sample'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'shap' has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 15\u001b[0m\n\u001b[0;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHVAC_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_base_dir\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     }\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m EnhancedHVACPipeline(config)\n\u001b[1;32m---> 15\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_enhanced_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mgenerate_recommendations(results)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Save all results\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[47], line 157\u001b[0m, in \u001b[0;36mEnhancedHVACPipeline.run_enhanced_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run enhanced pipeline with all new features\"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Run basic pipeline first\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m     base_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_data()\n",
      "Cell \u001b[1;32mIn[43], line 200\u001b[0m, in \u001b[0;36mHVACAnalysisPipeline.run_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m best_model \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mmodels[best_model_name]\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Use a sample from X_train_flat for background; adjust reshape as needed\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_explainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlstm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mcompute_shap_values(X_test_flat, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    202\u001b[0m explainer\u001b[38;5;241m.\u001b[39mplot_shap_summary(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor\u001b[38;5;241m.\u001b[39mget_feature_names(), X_test_flat)\n",
      "Cell \u001b[1;32mIn[37], line 16\u001b[0m, in \u001b[0;36mExplainableAI.create_explainer\u001b[1;34m(self, model, X_train, model_type, n_samples)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_explainer\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, X_train: np\u001b[38;5;241m.\u001b[39mndarray, model_type: \u001b[38;5;28mstr\u001b[39m, n_samples: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m     X_background \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m(X_train, n_samples)\n\u001b[0;32m     17\u001b[0m     predict_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model_wrapper(model, model_type)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoencoder\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'shap' has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'data_path': 'HVAC_dataset.csv',\n",
    "        'output_base_dir': 'outputs',\n",
    "        'power_cost_per_kwh': 0.15,  # Adjust based on local rates\n",
    "        'model_params': {\n",
    "            'sequence_length': 24,\n",
    "            'forecast_horizon': 12,\n",
    "            'batch_size': 32,\n",
    "            'epochs': 100             # Increased epochs for better training\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    pipeline = EnhancedHVACPipeline(config)\n",
    "    results = pipeline.run_enhanced_pipeline()\n",
    "    recommendations = pipeline.generate_recommendations(results)\n",
    "    \n",
    "    # Save all results\n",
    "    pipeline.save_results(results, 'enhanced_results.json')\n",
    "    pipeline.save_results(recommendations, 'enhanced_recommendations.json')\n",
    "    \n",
    "    # Log key findings\n",
    "    logging.info(f\"Optimal setpoint: {results['optimization_results']['optimal_setpoint']:.1f}Â°C\")\n",
    "    logging.info(f\"Potential daily savings: ${results['optimization_results']['daily_cost_savings']:.2f}\")\n",
    "    logging.info(f\"Number of anomalies detected: {results['anomalies']['total_count']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
